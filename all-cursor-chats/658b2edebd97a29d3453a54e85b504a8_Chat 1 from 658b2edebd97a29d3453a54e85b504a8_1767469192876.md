# Chat: Chat 1 from 658b2edebd97a29d3453a54e85b504a8

**Source:** 658b2edebd97a29d3453a54e85b504a8/aiService.prompts
**Messages:** 1
**Recovered:** 2026-01-03T19:39:52.876Z

---

## unknown

build this:

FILE 1: CONCEPTUAL FOUNDATIONS

Project: Project UP

Status: Canonical. Authoritative.

A.1 CORE SYSTEM DEFINITION (CANONICAL)

Status: Authoritative. Final wording. This replaces all earlier informal descriptions.

What this system is

This system is a deterministic, consequence-first simulation of work under pressure, designed to model how projects degrade, people burn out, and quality collapses after decisions appear reasonable and before failure becomes obvious.

It does not attempt to optimize outcomes.
It does not attempt to guide the user.
It does not attempt to explain itself by default.

It models what happens, not what should be done.

What problem it addresses

In real-world projects:

Harm accumulates silently

Stress outlives the situation that caused it

Recovery is slower than damage

Some states cannot be undone

Systems appear stable shortly before collapse

Most tools fail to represent this because they assume:

"If the user is smart enough, they can always fix things."

This system explicitly rejects that assumption.

Core premise (history > intent)

History matters more than intent.

The system is built so that:

Past overload affects future capacity

Short-term fixes can cause long-term damage

Late interventions cannot erase earlier costs

Consequences emerge from accumulated state, not isolated actions

What the system models (high level)

The system models:

Time as a sequence of irreversible steps

People as capacity-limited agents who accumulate stress

Tasks as work units that can degrade in quality under pressure

Dependencies as structural constraints that block progress

Burnout as a terminal loss of capacity

The system does not model motivation, morale, or intent. Only consequences.

What the system guarantees

The system guarantees that:

Damage accumulates faster than it heals

Burnout is irreversible within a session

Quality can degrade without immediate visibility

Removing pressure does not instantly restore health

Failure can occur without dramatic warning

If any of these guarantees are violated, the system is considered incorrect.

What the system deliberately avoids

The system deliberately avoids:

Optimization feedback loops

Performance metrics

Alerts and warnings

Scoring or success indicators

Prescriptive guidance

Explanatory narration during execution

These are excluded because they reduce resistance and allow gaming.

Intended mode of understanding

Understanding is expected to arise from:

Observation over time

Comparison between "looks fine" and "is fine"

Recognition of delayed effects

Realization that intervention has arrived too late

The system is not designed to teach quickly. It is designed to teach permanently.

Time Semantics (Canonical)

Time advances independently of user intent.
Pausing, waiting, or inaction does not neutralize cost.
Time is a force in the system, not a convenience or resource owned by the user.

User Role Constraint

The system does not model roles, authority, responsibility, or moral agency — it models consequence under constraint.

Anchor statement (single sentence)

This system models how pressure breaks projects quietly, deterministically, and irreversibly—often while everything still appears under control.

A.2 EXPLICIT NON-GOALS / REJECTIONS (CANONICAL)

Status: Authoritative. Final wording.

Purpose

This section exists to eliminate ambiguity.

Most systems drift not because of bad intent, but because of:

Helpful additions

Familiar patterns

Incremental conveniences

This document defines what this system will never become, regardless of how tempting it may feel later.

What the system is NOT

This is critical to understand.

This system is not:

A productivity tool

A project management dashboard

A planning assistant

A gamified simulator

An optimization engine

A teaching tool that explains itself

Those assumptions are explicitly rejected.

Rejected categories (tools, dashboards, simulators, etc.)

The following categories are permanently out of scope:

Project Management Tools

This is not Jira, Asana, or Trello

It does not help you "manage" anything

It does not optimize for delivery

Dashboards

No health indicators

No traffic light systems

No KPIs or metrics for "project health"

Gamified Simulators

No scoring

No win conditions

No replay-until-you-get-it-right

Teaching Tools

No tutorials

No hints

No "best practices" guidance

Optimization Engines

No suggestions

No auto-balancing

No "what-if" scenario planning with undo

Rejected design patterns

The following patterns are prohibited:

Feedback Loops

No "you're doing well" or "you're failing"

No progress bars toward success

No optimization metrics

Warning Systems

No alerts before thresholds

No "Bob is about to burn out" notifications

No preemptive guidance

Recovery Mechanics

No "heal" buttons

No undo/reset

No time reversal

Explanatory Narration

No real-time tooltips explaining "why"

No proactive help text

No guided tours

Friendly UX

No reassuring messages

No encouragement

No hand-holding

Canonical rejection test

If a proposed addition answers any of the following questions, it is rejected:

"How am I doing?"

"What should I fix?"

"What caused this?" (during execution)

"How can I recover faster?"

"How close am I to failure?"

The system does not answer these. It lets the user discover them too late.

What violating this looks like

Examples of violations:

❌ Adding a "Stress Alert" when someone approaches burnout
❌ Showing a tooltip: "Consider reassigning this task"
❌ Adding a "Project Health Score"
❌ Implementing an undo button
❌ Auto-suggesting optimal resource allocation
❌ Displaying "You're 3 steps from failure" warnings

Each of these makes the system easier to use at the cost of making it untrue.

Anchor statement

Anything that reduces ambiguity, reversibility, or delayed consequence is out of scope for this system.

A.3 RESISTANCE (FORMAL DEFINITION)

Status: Authoritative. Final wording.

Definition

Resistance is the property of a system that prevents users from cheaply correcting, optimizing, or escaping the consequences of their past actions.

In this system, resistance ensures that:

Harm persists after its cause is removed

Recovery is slower than damage

Late insight does not equal reversal

Correct understanding does not guarantee control

Resistance is not an obstacle. It is a truth-preserving constraint.

What Resistance Is NOT

It is critical to distinguish resistance from superficially similar concepts.

Resistance ≠ Difficulty

Difficulty tests skill

Resistance tests irreversibility

Adding complexity or harder puzzles does not create resistance.

Resistance ≠ Friction

Friction slows interaction

Resistance refuses erasure

Adding steps, confirmations, or complexity does not create resistance if the system can still be restored to a clean state.

Resistance ≠ Punishment

Punishment is reactive and explicit

Resistance is structural and indifferent

The system does not penalize the user. It simply remembers.

Why Resistance Is Essential

Without resistance, the system collapses into one of the following failure modes:

A simulator that can be played correctly

A dashboard that rewards attentiveness

A puzzle that resets after mistakes

A training tool that teaches avoidance, not consequence

Real-world systems do not behave this way.

In reality:

Stress outlives the crisis

Quality debt appears later

People do not instantly recover

Understanding often arrives too late

Resistance is what prevents the model from lying.

How Resistance Manifests in This System

Resistance is enforced structurally, not theatrically.

Key manifestations include:

Asymmetric dynamics

Damage accumulates faster than it decays

Irreversible states

Certain thresholds (e.g. burnout) permanently alter capacity

Delayed visibility

Harm becomes visible only after it has already shaped outcomes

Silent degradation

No alerts, no warnings, no explanatory narration

Limited control surfaces

The user cannot dial back history

Resistance exists even when the user understands the system perfectly.

The Core Resistance Test (Canonical)

A system is considered resistant only if the following condition holds:

The user can make the system look healthy for a while while unknowingly making its eventual failure inevitable.

If this condition is not true, resistance has been compromised.

Examples of Resistance (Conceptual)

These are conceptual illustrations, not implementation prescriptions.

Example 1:

Removing overload does not remove accumulated stress

Example 2:

Reducing task pressure does not restore lost quality

Example 3:

Reassigning work does not revive burned-out capacity

Example 4:

Seeing deterioration does not grant the ability to prevent it retroactively

In each case, the system allows temporary stabilization but not historical repair.

What Violates Resistance (Explicit)

The following changes break resistance and are therefore invalid:

❌ Undoing irreversible states
❌ Auto-balancing to help the user recover
❌ Warning systems that prevent crossing thresholds
❌ Metrics that expose hidden state early
❌ Guidance that teaches avoidance strategies
❌ Explanations that collapse ambiguity in real time

If a change makes failure avoidable once understood, resistance has been lost.

Resistance as a Design Filter

For every future addition, the following question must be asked:

"Does this make it easier for the user to escape the cost of past decisions?"

If the answer is yes, the change must be rejected or redesigned.

Resistance always takes precedence over usability, clarity, or comfort.

Anchor Statement

Resistance ensures that understanding does not equal control, and that consequences outlive their causes.

A.4 SILENCE AS A DESIGN PRINCIPLE

Status: Authoritative. Canonical. Non-negotiable.

Formal Definition

Silence is the deliberate absence of explanation, narration, guidance, or warning during system operation.

In this system, silence means:

No real-time explanations

No alerts or prompts

No diagnostic messages

No guidance about what is happening or why

Silence is not a missing feature. It is an active design choice.

Why Silence Is Necessary

Real systems fail quietly.

In reality:

Damage accumulates without announcement

Signals are ambiguous

Understanding arrives late

Clarity often coincides with irreversibility

Systems that explain themselves continuously create a false model of reality—one where:

Insight arrives on time

Intervention is always possible

Harm is preventable once noticed

Silence preserves truth by withholding clarity until it is structurally too late.

Silence vs. Opacity (Important Distinction)

Silence is often confused with opacity. They are not the same.

Opacity: Hides information arbitrarily
Silence: Allows information to exist but refuses to interpret it

In this system:

State is visible

Consequences are observable

Interpretation is withheld

The system does not obscure outcomes—it refuses to explain them.

What Silence Prevents

Silence blocks several failure modes common in interactive systems:

Early intervention bias

Users cannot act simply because they were warned

Optimization gaming

Users cannot tune behavior against exposed thresholds

Narrative closure

Users cannot rely on the system to tell the story

False reassurance

The absence of alerts avoids the implication of safety

Silence keeps ambiguity intact.

How Silence Manifests in This System

Silence is enforced through multiple constraints:

No alerts, warnings, or notifications

No tooltips explaining degradation

No textual narration of cause-and-effect

No labels like "healthy," "at risk," or "critical"

No confirmation that a choice was good or bad

The system shows but does not tell.

The Timing Principle

Silence is primarily about when understanding is allowed.

In this system:

Understanding is delayed

Explanation, if ever introduced, is retrospective

Insight arrives after consequences have already shaped outcomes

This preserves the core truth:

Recognition does not guarantee prevention.

What Violates Silence (Explicit)

The following additions violate silence and are prohibited:

❌ Warning banners
❌ "Risk increasing" indicators
❌ Status summaries ("Project health")
❌ Inline explanations ("This happened because...")
❌ Predictive alerts ("If you continue...")
❌ Comforting messages ("Everything is fine")

Any feature that collapses ambiguity in real time breaks silence.

Silence as an Integrity Check

For any proposed feature, apply this test:

"Does this help the user understand what is happening before the consequences have fully materialized?"

If yes, silence has been violated.

Silence takes precedence over clarity, usability, and reassurance.

Anchor Statement

Silence ensures that the system reveals consequences through experience, not explanation.

A.5 IRREVERSIBILITY RULES

Status: Authoritative. Canonical. Non-negotiable.

Formal Definition

Irreversibility is the property that certain system states, once entered, cannot be exited within the same simulation context.

In this system, irreversibility ensures that:

Time has direction

History has weight

Late correction does not erase early cost

Recovery is bounded, asymmetric, or impossible

Irreversibility is not punitive. It is structural truth.

Why Irreversibility Is Required

Real-world systems do not allow full reversal.

In reality:

People burn out and do not instantly recover

Quality degrades in ways that cannot be fully repaired

Trust, health, and capability have thresholds

Insight often arrives after capacity is already lost

A system that allows reversal after recognition creates a false model of causality—one where understanding equals control.

This system explicitly rejects that model.

Irreversible States (Conceptual)

The following classes of state are irreversible by design:

Burnout

Once a person enters a burned-out state:

Their effective capacity is permanently reduced or eliminated

Removing workload does not restore function

Reassignment does not recover lost capability

Burnout is not a meter to be refilled. It is a boundary.

Accumulated Stress Effects

While stress levels may decay slowly, their secondary effects are not fully reversible:

Increased fragility

Reduced effective throughput

Heightened sensitivity to future overload

The system remembers prior stress even after conditions improve.

Quality Degradation

Once quality risk has accumulated:

Downstream effects persist

Later care cannot fully restore lost integrity

Consequences may emerge after apparent completion

Quality debt behaves like structural damage, not surface wear.

Asymmetry of Change

Irreversibility is enforced through asymmetric dynamics:

Damage accumulates quickly

Recovery occurs slowly or not at all

Threshold crossings matter more than continuous values

Early decisions weigh more than late corrections

This asymmetry ensures that:

Fixing things later is not equivalent to not breaking them earlier.

What Irreversibility Does NOT Mean

To avoid misinterpretation:

Irreversibility does not mean:

Instant collapse

Punishment for small mistakes

All outcomes are predetermined

It means that:

Some losses are permanent once incurred, even if the system continues operating.

Why Undo, Reset, and Recovery Are Prohibited

The following features explicitly violate irreversibility and are disallowed:

❌ Undoing past actions
❌ Resetting people to prior capacity
❌ Healing burnout through "rest mechanics"
❌ Retroactively improving quality
❌ Time rewind or rollback

These features erase history and invalidate consequence.

Irreversibility as a Design Gate

For any proposed addition, apply this test:

"Does this allow the system to return to a prior effective state without paying the full historical cost?"

If yes, the feature must be rejected.

Irreversibility overrides usability, fairness, and comfort.

Relationship to Resistance and Silence

Irreversibility is inseparable from the other core principles:

Resistance ensures consequences cannot be cheaply escaped

Silence ensures irreversibility is not preemptively signaled

Irreversibility ensures history cannot be rewritten

Together, they form the system's ethical and structural core.

Anchor Statement

Irreversibility ensures that time matters, history persists, and some costs cannot be paid retroactively.

End of File 1: Conceptual Foundations

FILE 2: ARCHITECTURAL INVARIANTS
Status: Authoritative. Non-negotiable.

Purpose: This file defines the strict role and limits of each architectural layer (Engine, Display Adapter, UI), what each layer is allowed and forbidden to do, and how stages must respect boundaries.

B.1 ENGINE INVARIANTS
What the Engine Is
The engine is the authoritative source of truth for all simulation state.

It is:

Minimal (contains only what is necessary for consequence modeling)

Silent (produces no explanations, logs, or narratives by default)

Deterministic (same inputs → same outputs)

Consequence-first (models what happens, not what should happen)

What the Engine Must NOT Contain
The engine must never include:

❌ Display metadata (names, labels, colors, layout hints)
❌ UI-specific logic (tooltips, warnings, explanations)
❌ Narrative structures (events, stories, logs)
❌ Optimization hooks (suggestions, guidance, scoring)
❌ Recovery mechanics (undo, reset, heal)

If the engine knows how it will be shown, it has been corrupted.

Engine State Rules
Engine state must be:

Minimal: Only properties required for consequence propagation

Authoritative: UI cannot override or modify engine state

Silent: No strings, messages, or explanatory fields

Example violations:

Adding taskName to TaskState (belongs in Display Adapter)

Adding warningMessage to PersonState (violates silence)

Adding healthScore to ProjectState (violates resistance)

Engine Logic Rules
Engine logic must:

Advance time deterministically

Accumulate stress asymmetrically (faster up, slower down)

Make burnout irreversible

Degrade quality silently

Respect dependency constraints

Engine logic must NOT:

Explain why something happened

Suggest what the user should do

Optimize for "good" outcomes

Provide recovery paths

Canonical Engine Test
An engine is valid only if:

Removing all UI and Display Adapter code does not prevent the engine from running to completion.

If engine execution depends on UI or display logic, the engine has been corrupted.
​

B.2 UI INVARIANTS
What the UI Is
The UI is a read-only rendering layer that reflects engine state without altering it.

It is:

Observational (shows consequences, does not control them)

Silent (does not explain by default)

Honest (degrades when the system degrades)

What the UI Must NOT Do
The UI must never:

❌ Modify engine state directly
❌ Compute derived consequences (stress, quality risk, burnout) — engine does this
❌ Add explanations or warnings during execution
❌ Provide optimization feedback ("you're doing well")
❌ Invent data when display mappings are missing

The UI renders only. It does not decide.

UI-Engine Relationship
Information flow is strictly one-directional:

text
Engine → Display Adapter → UI
The UI may:

Read engine state

Join engine state with Display Adapter metadata (by ID)

Render visual consequences (color, opacity, motion)

The UI must NOT:

Write to engine state

Bypass the Display Adapter

Fill in missing data with guesses

Visual Consequence Rules
The UI may show consequences through:

Color changes (warmer = overload, muted = burnout)

Opacity (fading = degradation)

Subtle motion (pulse = instability, freeze = burnout)

Absence (removed affordances when burned out)

The UI must NOT show consequences through:

Text explanations ("Bob is overloaded")

Numeric indicators ("Stress: 85%")

Warning icons or alerts

Traffic light systems (red/yellow/green)

Anchor Statement
The UI exists to make consequence visible without making it actionable.
​

B.3 DISPLAY ADAPTER INVARIANTS
Purpose
The display adapter is a read-only bridge between engine state (IDs, minimal data) and UI presentation (names, labels, layout hints).

It exists to:

Translate engine IDs into human-readable labels

Provide layout metadata for UI rendering

Keep engine state minimal

It does NOT:

Add behavior

Compute consequences

Interpret meaning

Read-Only Guarantee
The display adapter must be:

Static (defined once, does not change during execution)

Deterministic (same ID → same label, always)

Read-only (never writes to engine)

Explicit Mapping Only
All display mappings must be explicit and declared.

Allowed:

Task IDs → task names

Person IDs → person labels

Dependency IDs → layout hints

NOT allowed:

Deriving importance from task properties

Inferring priority from deadline proximity

Generating warnings based on stress levels

No Logic Leakage
The display adapter must NOT contain:

Conditionals based on engine thresholds

Logic that interprets severity

Transformations that imply evaluation

If display behavior changes based on engine values, it belongs in the UI, not the Display Adapter.
​

No Bidirectional Flow
Information flow must be strictly one-directional:

text
Engine → Display Adapter → UI
The display adapter must never:

Send signals back to the engine

Influence state transitions

Act as a mediator or controller

Any bidirectional coupling violates this invariant.
​

Separation of Concerns
The engine must have:

No knowledge of display structures

No presentation metadata

The display adapter must:

Consume engine state

Expose display metadata only

Never influence engine behavior

Simulation truth must not depend on how it is shown.
​

Prohibited Display Adapter Capabilities
The following are permanently disallowed in the display adapter:

Dynamic computation

State aggregation

Caching with behavioral meaning

Derived metrics

Conditional rendering rules

Visual consequence logic

The display adapter is not where meaning emerges.
​

Canonical Display Adapter Test
A display adapter is valid only if:

Removing the display adapter entirely does not change engine behavior or simulation outcomes in any way.

If removal alters behavior, the adapter has violated its role.
​

Anchor Statement
The display adapter exists to name what already exists, not to decide what it means.
​

B.4 STAGE BOUNDARY RULES
Purpose
Stages exist to prevent:

Premature UX influence on logic

Narrative creep (events, explanations, dashboards)

Optimization surfaces appearing too early

Each stage locks invariants before moving on. Later stages cannot modify earlier-stage guarantees.
​

No Temporal Leakage
Concepts, logic, or affordances from later stages must NOT appear early, including:

Interactions (before Stage 3)

Explanations (before Stage 5)

Metrics (never)

Optimization hooks (never)

Recovery mechanics (never)

If a feature will be needed later, it must still be excluded until its stage is active.
​

No Conceptual Preloading
Anticipatory abstractions are prohibited

Placeholder logic for future stages is prohibited

"We'll need this later" is not a valid justification

Every construct must be justified by the current stage only.
​

What Constitutes a Stage Violation
A stage violation occurs if any of the following happen prematurely:

Engine behavior expands beyond its stage mandate

UI gains interpretive or explanatory power

Display logic influences behavior

Reversibility is introduced early

Optimization surfaces appear

Narrative framing emerges

Stage violations invalidate the build regardless of local correctness.
​

Frozen vs. Mutable
At any given stage:

Frozen stages cannot be reopened without explicit justification

Current stage is mutable within its defined scope

Future stages do not exist yet

Example:

At Stage 2: Stage 0 and Stage 1 are frozen

At Stage 3: Stages 0-2 are frozen

B.5 WHAT IS NOW PERMANENTLY FROZEN
These are no longer negotiable:

✅ TaskState will never carry names or dependencies again
✅ PersonState will never carry display labels
✅ Display data will never drive logic
✅ UI will never explain behavior by default

If you ever feel tempted to break one of these, that's a signal to stop.
​

End of File 2: Architectural Invariants

FILE 3: STATE & SYSTEM MODEL (CONCEPTUAL)

Status: Canonical. Authoritative.

Purpose

Define the canonical state model and system dynamics the engine operates on.
This file specifies what exists, what can change, and how change propagates without introducing narration, optimization, or UI concerns.

This file answers: What is the system, formally?

C.1 CANONICAL STATE DEFINITIONS
Purpose

This section defines the complete and minimal set of state objects that may exist within the system.

Anything not defined here is not part of the system, regardless of usefulness or intuition.

State exists only to support consequence. No state exists for explanation, optimization, or comfort.

General State Principles
All state must be:

Explicit

Finite

Enumerable

No implicit, inferred, or hidden state is permitted.

State must be representable at any time as a complete snapshot. There is no background or derived state.

State Closure Rule
The engine must be able to:

Serialize the entire system state

Restore it exactly

Continue evolution without loss of meaning

If restoration alters future behavior, the state model is invalid.

Task State
A Task represents a unit of work under constraint.

A task may contain only the following simulation-relevant properties:

id
A unique, stable identifier.

status
One of a closed set of valid task states:
pending · inProgress · blocked · completed

assignedTo (optional)
A reference to a Person identifier. Absence of assignment does not halt time or consequence.

qualityRisk
A non-negative scalar representing accumulated degradation risk.

No other task properties are permitted in engine state.

Person State
A Person represents a finite capacity actor subject to pressure.

A person may contain only the following properties:

id
A unique, stable identifier.

stress
A bounded scalar representing accumulated pressure.

availability
One of a closed set of discrete states:
normal · overloaded · burnedOut

workload
A scalar representing current applied work demand.

Person state exists to model limitation, not performance.

Project State
A Project represents the enclosing system state.

A project may contain only:

currentTime
A monotonically increasing scalar.

tasks
A collection of Task states indexed by task identifier.

people
A collection of Person states indexed by person identifier.

Project state is the complete system snapshot.

Canonical State Test
The state model is correct only if:

Two identical state snapshots evolved under identical time advancement always produce identical future states.

If hidden state exists, this condition fails.

C.2 TIME MODEL
Purpose

This section defines what time is within the system and how it constrains all state evolution.

Time is not a convenience, interface affordance, or scheduling tool. It is a primary force acting on all entities.

Monotonic Advancement
Time must advance monotonically.

Time must never:

Decrease

Pause

Reset

Rewind

All state transitions are evaluated strictly forward in time.

Backward or reversible time mechanics are prohibited.

Uniform Application
Time advancement applies uniformly to:

All tasks

All people

All state transitions

No entity may be exempt from time's effects.

Selective suspension is prohibited.

No Temporal Lookahead
The engine must not:

Predict future states

Simulate ahead without committing

Expose future consequences early

Time reveals consequence only after it has occurred.

Time as Independent Force
Time advances independently of:

User intent

System state

Completion status

Time does not wait for the user to be ready.

Pausing, waiting, or inaction does not neutralize cost. Time continues to apply pressure, accumulate stress, and degrade quality regardless of user action or inaction.

Time Granularity
Time advances in discrete, uniform steps.

Each step represents one unit of project time (conceptually: a day, a sprint, a work period—the specific mapping is implementation-dependent but must remain consistent).

All consequence propagation occurs per step.

Relationship to State Transitions
Time advancement triggers:

Stress accumulation (if overloaded)

Stress decay (if not overloaded)

Quality risk accumulation (if tasks remain under pressure)

Burnout state transitions (if stress thresholds are crossed)

Task blocking/unblocking (based on dependencies and person availability)

Time does not directly cause these transitions—it creates the conditions under which they occur.

Prohibited Time Mechanics
The following are permanently disallowed:

Time reversal or rollback

Selective time advancement

Time dilation for specific entities

Time as a resource (spend/save)

User-controlled pauses that prevent consequence

Time is a constraint, not a tool.

Anchor Statement
Time is the forcing function that makes history irreversible and late intervention insufficient.

C.3 WORKLOAD & CAPACITY MODEL
Status: Authoritative. Non-negotiable.

Purpose

This section defines how work is applied and capacity is constrained.

It answers: When does pressure begin?

Workload Definition
Workload is the cumulative demand placed on a person at any given time.

Workload is determined by:

The number of tasks currently assigned to that person

The status of those tasks (only inProgress tasks contribute to workload)

Workload is not:

A measure of task difficulty

A measure of time remaining

A subjective assessment

A user-configurable value

Workload is an objective count of concurrent active tasks.

Capacity Definition
Capacity is the maximum number of tasks a person can handle simultaneously without entering an overloaded state.

Capacity is fixed per person.

Capacity does not:

Increase with experience

Decrease temporarily (except through burnout)

Vary based on task type

Respond to motivation or encouragement

Capacity is a structural limit, not a performance metric.

Overload Condition
A person enters an overloaded state when:

workload > capacity

Once overloaded:

Stress begins to accumulate (per time step)

The person remains functionally available (can still work)

Quality risk increases for all assigned tasks

The person does not immediately fail

Overload is a pressure state, not a failure state.

Normal Condition
A person is in a normal state when:

workload ≤ capacity

In the normal state:

Stress decays slowly (if any stress exists)

Quality risk does not increase from overload

The person operates within sustainable limits

Normal does not mean "optimal"—it means not overloaded.

Assignment Semantics
When a task is assigned to a person:

That person's workload increases immediately

If the assignment pushes workload above capacity, the person becomes overloaded

The assignment does not fail or get rejected

The system does not prevent overload. It models the consequences of overload.

Unassignment Semantics
When a task is removed from a person (completed, reassigned, or blocked):

That person's workload decreases immediately

If workload drops to or below capacity, the person returns to normal state

Accumulated stress does not disappear

Removing workload stops further stress accumulation but does not reverse damage already done.

Capacity and Burnout
When a person enters the burnedOut state:

Their effective capacity becomes zero

They can no longer contribute to any task

All assigned tasks become blocked

Workload no longer affects them (they are functionally removed from the system)

Burnout is not temporary overload—it is permanent incapacitation.

No Dynamic Capacity Adjustment
The following are prohibited:

Increasing capacity through "training" or "motivation"

Temporarily boosting capacity for "crunch time"

Reducing capacity as a penalty

Variable capacity based on task importance

Capacity that "recovers" after rest

Capacity is constant until burnout makes it zero.

Workload Does Not Equal Severity
Workload measures concurrent demand, not total work or difficulty.

A person with 1 extremely difficult task has workload = 1.
A person with 3 trivial tasks has workload = 3.

The system models constraint, not effort.

Anchor Statement
Workload and capacity define when pressure begins, but not when it ends—stress outlives overload.

C.4 STRESS ACCUMULATION & DECAY MODEL
Status: Authoritative. Non-negotiable.

Purpose

This section defines how stress accumulates, decays, and persists.

It answers: How does pressure damage people, and why doesn't it go away quickly?

Stress Definition
Stress is a scalar value representing accumulated pressure on a person.

Stress is:

Bounded (minimum 0, maximum threshold)

Persistent (does not reset between time steps)

Cumulative (builds over time under sustained pressure)

Stress is not:

A momentary feeling

Binary

Reversible through willpower

Visible as a precise number in the UI

Stress is the hidden cost of overload.

Stress Accumulation
Stress accumulates when a person is in the overloaded state.

Per time step:

If workload > capacity, stress increases by a fixed increment.

Accumulation is:

Automatic

Continuous

Deterministic

The user cannot prevent stress accumulation while a person remains overloaded.

Stress Decay
Stress decays when a person is in the normal state.

Per time step:

If workload ≤ capacity, stress decreases by a fixed decrement.

Decay is:

Slower than accumulation (asymmetric)

Gradual

Incomplete (secondary effects persist)

Removing overload does not immediately restore health.

Asymmetry (Critical)
Stress accumulates faster than it decays.

Example:

3 steps overloaded → +15 stress

15 steps normal → -15 stress to recover fully

Consequences:

Short overload bursts create long recovery periods.

Sustained overload can never be fully recovered from before burnout.

Late intervention is structurally insufficient.

This asymmetry is not tunable for “balance”—it is a truth constraint.

Stress Thresholds
Stress has critical thresholds:

Overload-related threshold (conceptual): supports overloaded dynamics.

Burnout threshold: when exceeded, triggers burnedOut (irreversible).

Thresholds are:

Fixed

Hidden

Unforgiving (no warnings)

Stress Persistence
Stress persists across state changes.

Example:

Overloaded → stress accumulates.

Workload reduced → normal, stress begins decaying from current level.

Secondary effects:

Increased fragility

Reduced effective throughput (conceptual)

Heightened sensitivity to future overload

The system remembers prior stress even after conditions improve.

No Stress Relief Mechanics
Prohibited:

Manual stress reduction

Stress reset after completing tasks

Bonuses for "good" decisions

Instant recovery time-outs

Caps that prevent burnout

Stress can only be reduced through sustained periods of normal workload.

Stress and Burnout (Link)
Once stress crosses the burnout threshold:

Person enters burnedOut.

Stress is no longer relevant; person is incapacitated.

Workload changes cannot restore them.

Burnout is not high stress—it is a discrete, irreversible transition.

Anchor Statement
Stress accumulates quickly, decays slowly, and persists after its cause is removed—this asymmetry makes late intervention structurally insufficient.

C.5 BURNOUT & IRREVERSIBILITY MODEL
Status: Authoritative. Non-negotiable.

Purpose

This section defines discrete state transitions related to burnout.

It answers: When does damage become permanent?

Burnout Definition
Burnout is a discrete, irreversible state entered when accumulated stress exceeds a critical threshold.

Burnout is not:

Temporary

A high stress level

Reversible

A pre-warning

Burnout is a boundary, not a meter.

Burnout Trigger Condition
A person transitions to burnedOut when:

stress ≥ burnoutThreshold

Once this occurs:

Transition is immediate.

It cannot be prevented by later actions.

It cannot be undone.

Burnout is the result of accumulated history, not a single moment.

Availability States
A person exists in one of three availability states:

normal
Workload ≤ capacity; stress may exist but decays; can work.

overloaded
Workload > capacity; stress accumulates; can still work; quality degrades.

burnedOut
Stress exceeded threshold; cannot work; assigned tasks blocked; permanent.

State transitions flow only: normal → overloaded → burnedOut.

Irreversibility (Core Rule)
Once burnedOut:

They cannot return to normal.

They cannot return to overloaded.

Reducing workload does not restore them.

Waiting does not heal them.

Reassignment does not restore capacity.

Burnout is not recoverable within the simulation.

Functional Consequences of Burnout
When a person becomes burnedOut:

Effective capacity becomes zero.

They cannot accept new tasks.

All tasks assigned become blocked (until reassigned).

Workload becomes irrelevant.

They remain visible but inactive—persistent evidence of failure.

No Recovery Paths
Prohibited:

"Rest" mechanics

Time-based recovery

Healing actions

Partial capacity recovery

Temporary debuffs modeled as burnout

Once burnout occurs, the only option is to continue without that person.

Why Burnout is Irreversible
Burnout represents:

Accumulated cost of sustained overload.

A threshold where damage cannot be undone.

Real-world truth that some losses are permanent.

Allowing recovery would:

Make stress reversible.

Allow “undo via waiting”.

Turn burnout into a temporary setback.

Violate core irreversibility.

Burnout is the enforcement mechanism for consequence.

Burnout vs. Overload
State	Reversible?	Can work?	Stress behavior
normal	N/A	Yes	Decays slowly
overloaded	Yes	Yes	Accumulates quickly
burnedOut	No	No	Irrelevant (beyond)
Overload is the unseen warning. Burnout is the irreversible outcome.

Burnout Threshold (Hidden)
The burnout threshold is:

Fixed.

Hidden.

Silent (no alerts).

The user discovers it only by crossing it.

Multiple Burnouts
Multiple people can burn out independently.

If all people burn out:

Project cannot progress.

All tasks effectively blocked.

Time still advances.

This represents total project failure without explicit "game over".

Failure is not signaled; it is discovered.

Anchor Statement
Burnout is not high stress—it is a discrete, irreversible state transition that represents permanent loss of capacity.

C.6 QUALITY DEGRADATION MODEL
Status: Authoritative. Non-negotiable.

Purpose

Define how quality degrades under pressure, accumulates silently, and persists beyond completion.

It answers: How does work quality fail quietly, and why can't it be fixed later?

Quality Risk Definition
Quality risk is a non-negative scalar representing accumulated degradation in a task.

Quality risk is:

Accumulated

Persistent

Silent

Structural

Quality risk is not:

Completion percentage

Visible defect count

Reversible

Binary

Quality risk is hidden damage that emerges later.

Quality Risk Accumulation
Quality risk accumulates when a task is worked on by an overloaded person.

Per time step:

If status = inProgress

And assigned person is overloaded

Then qualityRisk increases by a fixed amount.

Accumulation is automatic, continuous, silent, and irreversible.

Work done under pressure creates hidden debt.

When Quality Risk Does Not Accumulate
Quality risk does not accumulate when:

Task is pending

Task is blocked

Task is completed

Task is assigned to a person in normal state

Quality degradation requires both active work and pressure.

Quality Risk Persistence
Once accumulated:

It does not decay.

It does not disappear when the person returns to normal.

It does not reset when the task is reassigned.

It remains after completion.

Quality debt is permanent.

Delayed Visibility
Quality risk is not visible in real-time.

The user:

Cannot see quality risk values during execution.

Cannot easily distinguish high-risk vs low-risk tasks while working.

Discovers quality problems after decisions have shaped outcomes.

Quality degradation is discovered retroactively, not prevented proactively.

Quality Risk and Task Completion
A task can be marked completed regardless of qualityRisk.

Thus:

High-risk tasks can appear "done."

Completion does not guarantee integrity.

"Looks finished" ≠ "is sound."

The system allows degraded work to complete; it does not block it.

Downstream Consequences (Conceptual)
Once a high-quality-risk task is completed:

Downstream effects conceptually persist.

Future tasks may depend on compromised work.

Rework cannot fully restore integrity.

Future stages may surface this more explicitly. For now, quality risk is accumulated state.

No Quality Recovery Mechanics
Prohibited:

Quality decay

"Fix quality" actions

Reassignment-based quality improvement

Resets after completion

Bonuses that reduce risk

Quality risk, once accumulated, is permanent.

Quality Risk vs Task Status
Status	Accumulate risk? (if overloaded person)	Visible?
pending	No	Defined, not active
inProgress	Yes	Active work
blocked	No	Paused
completed	No (risk frozen)	Appears done
Why Quality Risk is Silent
Immediate visibility would:

Enable optimization around quality.

Allow avoidance of damage once understood.

Break resistance.

Silence preserves the truth that harm is often invisible until irreversible.

Quality risk models structural damage, not cosmetic defects.

Anchor Statement
Quality degrades silently under pressure, persists after apparent completion, and cannot be fully restored—later care cannot undo earlier compromise.

C.7 TASK PROGRESSION MODEL
Status: Authoritative. Non-negotiable.

Purpose

Define how tasks move through states under constraint.

It answers: How does work advance, stall, or quietly fail to?

Valid Task States
A task may exist in exactly one state:

pending – Defined but not started.

inProgress – Actively being worked on.

blocked – Cannot progress due to constraints.

completed – Work on this task is finished.

No additional task states are permitted.

Allowed State Transitions
Valid transitions:

pending → inProgress

inProgress → blocked

blocked → inProgress

inProgress → completed

No direct transitions:

From completed to any other state.

From blocked to completed.

From completed back to pending or inProgress.

Once completed, a task is structurally finished.

Preconditions for inProgress
A task may transition to inProgress only if:

Currently pending or blocked.

Assigned to a valid person (assignedTo is set).

All dependency constraints (if modeled) are satisfied.

The engine does not enforce “safe” conditions like non-overload. It allows unsafe work and models the consequences.

Blocking Semantics
A task transitions to blocked when:

Its assigned person becomes burnedOut, or

A required dependency becomes unavailable or invalid.

While blocked:

Task does not accumulate quality risk.

Task may or may not contribute to workload depending on model choice, but conceptually is inactive.

Project time and other consequences continue.

Blocked tasks can only leave blocked by returning to inProgress, never directly to completed.

Completion Semantics
A task transitions to completed only from inProgress.

On completion:

qualityRisk stops changing (frozen).

Task no longer contributes to workload.

No further state changes are allowed.

Completion does not validate quality; it only stops work.

No Rework State
There is no explicit "reopened" or "rework" state.

Rework must be modeled as:

A new task with its own id, status, and qualityRisk.

Possibly dependent on the original task.

Tasks do not reopen. History is not rewritten.

Time Interaction
Per time step:

pending / blocked: no quality risk accumulation.

inProgress:

Contributes to workload for its assigned person.

Accumulates quality risk if the person is overloaded.

completed: inert; persists only as evidence and frozen risk.

Task Model Integrity Rules
Every task must have a unique id.

status must always be one of the four valid states.

assignedTo may be empty only for pending or blocked.

A task assigned to a burnedOut person must be blocked or reassigned.

Any behavior outside these rules is invalid at the model level.

Anchor Statement
Tasks do not fail loudly; they quietly stall, complete under pressure, or freeze in place while their hidden quality debt persists.

End of File 3: State & System Model (Conceptual)

FILE 4: STAGE ROADMAP & EXECUTION

Status: Canonical. Authoritative.

Purpose: This file defines the complete stage progression from project inception to completion, what each stage builds and freezes, where the project currently is, and when to stop building.

D.1 COMPLETE STAGE MAP (OVERVIEW)

Purpose

This section provides a complete map of all stages from project inception to completion.

Each stage defines:

What is built

What is frozen afterward

What success looks like at that stage

This map exists to prevent infinite building and scope creep.

The Complete Stage Sequence

This project consists of 6 discrete stages (0–5).

Each stage is bounded, testable, and irreversible once completed.

Stage	Name	Purpose
0	Repository Scaffold	Structural foundation, no behavior
1	Core Simulation Engine	Truthful internal behavior, consequences, irreversibility
2	UI Wiring (Read-Only)	Make consequences visible without giving control
3	Interaction Layer	Allow limited interventions (reassign, adjust deadlines)
4	Retrospective Explainability	Post-session analysis and consequence tracing (Optional)
5	Refinement & Closure	Final calibration, visual polish, done state
Stage Progression Rules

Forward-only progression:

Stages advance in order (0 → 1 → 2 → 3 → 4 → 5)

Stages cannot be skipped

Earlier stages are frozen unless explicitly reopened

Frozen stages:

Once a stage is marked complete, its invariants are locked

Changes to frozen stages require explicit justification

Reopening a frozen stage signals architectural drift

Current stage scope:

Only the current stage may be actively modified

Future stages do not exist yet (no preloading)

Past stages provide constraints, not suggestions

Why This Map Exists

Without a stage map:

The project expands indefinitely

Features leak between stages

Architectural discipline erodes

"Just one more thing" becomes permanent

This map defines where the project ends.

What Happens After Stage 5

After Stage 5:

The project is considered complete

No new features are added

No new stages are created

The system exists as a finished artifact

"Complete" does not mean "perfect"—it means "done."

Stage Dependency Structure

text
Stage 0: Foundation
   ↓
Stage 1: Engine (depends on 0)
   ↓
Stage 2: UI Wiring (depends on 1)
   ↓
Stage 3: Interaction (depends on 2)
   ↓
Stage 4: Explainability (optional, depends on 3)
   ↓
Stage 5: Closure (depends on 3 or 4)
Each stage builds on the frozen invariants of previous stages.

Anchor Statement

This map defines not just what will be built, but when to stop building.

D.2 STAGE 0: FOUNDATION & SETUP

Purpose

Establish the structural foundation and repository scaffold without implementing any behavior.

This stage exists to create a clean starting point that prevents improvisation later.

What Was Built

Repository structure:

React + TypeScript + Vite project

Defined folder structure (src/state, src/engine, src/ui)

Placeholder files for all major components

Basic build tooling (package.json, tsconfig, vite config)

Type definitions:

Empty interfaces for Task, Person, Project, Dependency

No logic, no defaults, just shape definitions

UI scaffold:

Empty component shells (ProjectView, SidePanel, CausalityStrip)

Static layout composition

No interactivity

State placeholder:

Hardcoded mock project (2-3 tasks, 2 people, 1 dependency)

Used only to prove wiring works

No simulation logic

Engine stubs:

Empty function signatures (e.g., advanceTime)

No implementation

What Was Explicitly Excluded

❌ Simulation logic
❌ Time advancement
❌ State propagation
❌ User interactions
❌ Visual feedback systems
❌ Any anticipatory abstractions

If behavior existed at this stage, Stage 0 failed.

What Gets Frozen

After Stage 0 completion, the following become locked:

✅ Repository folder structure
✅ Core file responsibilities (state vs engine vs UI)
✅ Build tooling (React, TypeScript, Vite)
✅ Separation of concerns (no mixing state/engine/UI logic)

These cannot be changed without explicit architectural justification.

Acceptance Criteria

Stage 0 is complete only if:

npm install && npm run dev works without errors

App renders without console errors

All folders and files exist as specified

No business logic is present

No interactions exist

State is hardcoded and readable

Engine functions are stubs only

If any real logic exists, Stage 0 is incomplete.

Why This Stage Matters

Stage 0 establishes:

Separation of concerns (engine truth vs UI presentation)

Structural discipline (prevents mixing layers)

Clean slate (no legacy code to refactor later)

Explicit boundaries (makes drift visible)

Without Stage 0, later stages would feel improvised.

Anchor Statement

Stage 0 creates the container; Stage 1 fills it with consequence.

D.3 STAGE 1: CORE SIMULATION ENGINE

Purpose

Implement truthful internal behavior with memory, consequence propagation, and irreversibility—without any UI dependencies.

This stage exists to build an engine that models reality, not convenience.

What Gets Built

Time advancement:

advanceTime(state, steps) function that evolves the system forward

Monotonic time progression (no reversal, no pauses)

Uniform application across all entities

Stress accumulation & decay:

Stress increases when a person is overloaded (workload > capacity)

Stress decreases slowly when workload drops below capacity

Asymmetric dynamics: accumulation is faster than decay

Burnout mechanics:

Discrete state transition: normal → overloaded → burnedOut

Burnout occurs when stress exceeds a threshold

Burnout is irreversible within a session

Quality degradation:

qualityRisk accumulates on tasks worked on by overloaded people

Quality risk persists after task completion

Quality risk does not decay

Workload & capacity modeling:

Workload = count of inProgress tasks assigned to a person

Capacity = fixed limit per person

Overload condition: workload > capacity

Task progression:

Tasks move through states: pending → inProgress → blocked → completed

Tasks assigned to burned-out people become blocked

Blocked tasks do not accumulate quality risk

Dependency constraints:

Tasks respect dependency relationships

Blocked dependencies prevent downstream task progress

What Must Be Excluded

❌ UI logic (no visual representation)
❌ User interactions (no reassignment, no manual interventions)
❌ Explanations or narration (no event logs)
❌ Metrics or dashboards (no "health scores")
❌ Recovery mechanics (no undo, no healing)
❌ Optimization helpers (no suggestions)

The engine operates independently of how it will be shown.

Stage 1.1 Tightening Pass (Corrections)

Initial Stage 1 implementation may drift by introducing:

Progress percentages

Timestamp tracking

Event logs

Narrative structures

These must be removed to restore engine purity:

✅ Burnout converted to a discrete state (burnedOut)
✅ Burnout made irreversible
✅ Canonical state surface reduced (removed metrics)
✅ Event logs removed
✅ Engine made silent again

After tightening, the engine becomes harder to game and more realistic in failure behavior.

What Gets Frozen

After Stage 1 completion, the following become locked:

✅ Time is monotonic and irreversible
✅ Stress accumulation is asymmetric (faster up, slower down)
✅ Burnout is a discrete, permanent state
✅ Quality risk accumulates silently and persists
✅ Engine state is minimal and authoritative
✅ Engine does not know how it will be displayed

These are architectural invariants that cannot be violated.

Acceptance Criteria

Stage 1 is complete only if:

Engine can run without UI (purely functional)

Time advancement produces deterministic outcomes

Burnout occurs after sustained overload

Burnout is irreversible

Quality risk accumulates silently

Stress persists after overload is removed

No explanations or metrics are exposed

If the engine can be "optimized around" once understood, Stage 1 fails.

Why This Stage Matters

Stage 1 establishes:

Consequence over intent (history matters more than decisions)

Resistance (late insight doesn't equal control)

Irreversibility (some damage is permanent)

Silence (no warnings or explanations during execution)

Stage 1 is where the system becomes truthful instead of helpful.

Key Behavioral Guarantees

The engine guarantees:

Damage accumulates faster than it heals

Burnout is irreversible within a session

Quality can degrade without immediate visibility

Removing pressure does not instantly restore health

Failure can occur without dramatic warning

If any of these guarantees are violated, the engine is considered incorrect.

Anchor Statement

Stage 1 builds the truth engine; Stage 2 creates a window to observe it.

D.4 STAGE 2: UI WIRING (READ-ONLY)

Purpose

Wire the existing deterministic engine state into the React UI so that engine behavior becomes visible without adding new mechanics or control.

This stage exists to make consequences observable without making them preventable.

What Gets Built

State display:

React state holds the current ProjectState

UI reflects engine state in real-time

No UI component modifies engine state directly

Time control (minimal):

Single "Advance Time" button

Each click calls advanceTime(state, 1)

No auto-play, no variable speed, no pause-with-benefit

Visual consequence indicators:

People states:

normal → neutral appearance

overloaded → warmer color, subtle pulse/instability

burnedOut → muted, faded, visually inert

Task quality risk:

Rising risk → subtle visual degradation (blur, desaturation)

High risk → visible but not labeled

No numeric percentages shown

Task status:

pending → waiting state

inProgress → active visual state

blocked → visually constrained

completed → quiet, not celebratory

Display Adapter (architectural insertion):

Static, read-only bridge between engine IDs and human-readable labels

Provides task names, person names, dependency layout hints

Does not add logic, compute consequences, or interpret state

What Must Be Excluded

❌ New engine logic or mechanics
❌ User interactions that change state (reassignment, deadline changes)
❌ Explanations, tooltips, or warning text
❌ Numeric indicators ("Stress: 75%", "Quality: 60%")
❌ Event logs or narrative timelines
❌ Optimization feedback ("You're doing well")
❌ Recovery mechanics or undo buttons
❌ Drag-and-drop or click-to-modify

If user actions alter engine state beyond time advancement, Stage 2 fails.

Stage 2 Substages (Iterative Refinement)

Stage 2.0: Initial Wiring

Basic state-to-UI connection

Simple rendering of tasks and people

Initial time advancement

Display Adapter Insertion (Architectural)

Recognized need for clean separation of truth vs. labels

Introduced read-only Display Adapter layer

Preserved engine minimalism

Stage 2.1: Scope Correction Pass

Removed event/log concepts that crept in

Stripped TaskState back to simulation truth only

Restored display-only UI constraint

Stage 2.2: Visual Consequence Calibration

Tuning visual cues (color, opacity, motion)

Calibrating "feels wrong but not sure why" sensation

Ensuring silence is preserved (no explanatory text)

What Gets Frozen

After Stage 2 completion, the following become locked:

✅ Engine logic remains unchanged from Stage 1
✅ Display Adapter is read-only and static
✅ UI cannot invent or modify truth
✅ Time is the only user-controlled input
✅ Visual consequences are qualitative, not quantitative
✅ Silence is intentional (no explanations during execution)

These constraints protect resistance and irreversibility.

Acceptance Criteria

Stage 2 is complete only if:

Advancing time updates the UI visibly

Burnout appears visually after prior overload (not immediately)

Removing workload does NOT visually fix burnout

Quality risk is felt (through visual degradation) but not labeled

No new engine logic was added

No user interaction changes engine state beyond time advance

Someone watching for 30-60 seconds says: "Something is going wrong and I'm not sure I can stop it"

Critical test:

If they say "I need a tooltip" → explained too little/too late

If they say "I can fix this easily" → showed too much control

If they say "This is broken" → hid too much

Goal reaction: "Oh... this is serious" (not "cool" or "what's this for?")

Why This Stage Matters

Stage 2 establishes:

Observation without control (consequences are visible but not preventable)

Delayed understanding (clarity arrives after decisions have shaped outcomes)

Visual honesty (the UI degrades as the system degrades)

Resistance through constraint (limited control surfaces)

Stage 2 is where the system starts to feel real.

Key Design Principles

Silence:

No tooltips explaining "why"

No labels like "healthy" or "at risk"

No confirmations that choices were good/bad

Resistance:

Removing workload stops future damage but doesn't undo past damage

Visual improvements lag behind state improvements

Late intervention is visible but insufficient

Irreversibility:

Burned-out people stay visually dead

Quality degradation persists visually

History is visible in the current state

Anchor Statement

Stage 2 makes the system observable without making it controllable—you can watch it fail, but not prevent it.

D.5 STAGE 3: INTERACTION LAYER

Purpose

Allow a small set of controlled interventions (reassign tasks, adjust deadlines, modify dependencies) so the user can attempt to "fix" things—but the system quietly keeps score.

This stage exists to let the user act, then discover their actions were insufficient or too late.

What Gets Built

Task reassignment:

User can reassign a task from one person to another

Reassignment changes workload distribution

Reassignment does not restore burned-out people

Reassignment does not reduce accumulated quality risk

Deadline adjustments (conceptual):

User can extend or compress task timelines

Changing deadlines alters pressure distribution

Past stress does not disappear

Quality debt already accumulated persists

Dependency modifications (light):

User can soften or remove blocking dependencies

Unblocking allows tasks to proceed

Does not undo consequences of prior delays

Limited interaction scope:

No adding new tasks or people

No removing existing entities

No "heal" or "rest" actions

No time reversal or undo

What Must Be Excluded

❌ Recovery mechanics (healing burnout, reducing stress directly)
❌ Undo/rollback of past decisions
❌ Adding resources mid-session (new people, new time)
❌ Removing quality risk
❌ Explanations of why actions worked/failed
❌ Predictive feedback ("This will cause X")
❌ Optimization suggestions ("Try reassigning to Alice")

Interactions must feel insufficient, not empowering.

Key Design Constraint

Stage 3 interactions must preserve resistance:

Actions have cost: Reassigning work mid-sprint creates churn

Late intervention is structurally insufficient: By the time you act, damage is already done

Understanding ≠ control: Knowing what's wrong doesn't mean you can fix it

The user should feel:

"I'm trying to help but it's not enough"

"I should have done this earlier"

"Why isn't this working?"

NOT:

"I can optimize this"

"I figured out the puzzle"

"Now that I know the system, I can win"

What Gets Frozen

After Stage 3 completion, the following become locked:

✅ Interaction surface is limited (no expansion beyond defined actions)
✅ Actions do not erase history
✅ Late interventions remain insufficient
✅ Consequences persist after "correct" actions
✅ No guidance or suggestions appear

These constraints ensure resistance survives interactivity.

Acceptance Criteria

Stage 3 is complete only if:

User can reassign tasks and adjust constraints

Actions feel like interventions, not optimizations

Making "correct" moves doesn't prevent failure

Burnout still occurs despite intervention attempts

Quality risk accumulated earlier persists

No explanations appear during actions

The system still says "no" through consequence (not through blocking the action)

Critical test:

User tries to save a failing project

Realizes their actions came too late

Feels: "I should have acted 5 steps ago" (not "I need more information")

Why This Stage Matters

Stage 3 introduces:

Agency without control (you can act, but history still dominates)

Delayed consequence visibility (actions reveal problems, don't solve them)

Structural insufficiency (correct understanding ≠ successful outcome)

This is where resistance becomes undeniable.

Without Stage 3, the system feels like a movie (you watch but don't participate).
With Stage 3, the system feels like a trap (you participate but can't escape your past).

Interaction Philosophy

Stage 3 interactions are:

Reactive, not proactive (respond to problems, don't prevent them)

Local, not systemic (fix one thing, break another)

Delayed, not immediate (by the time you act, it's already late)

The user has control, just not enough control.

Anchor Statement

Stage 3 gives the user a steering wheel—but the car already left the road three turns ago.

D.6 STAGE 4: RETROSPECTIVE EXPLAINABILITY (OPTIONAL)

Purpose

Provide post-session analysis and consequence tracing—explaining what happened and why, but only after the simulation has completed or reached an irreversible state.

This stage exists to allow understanding without enabling prevention.

What Gets Built

Post-session review mode:

Activated only after the session ends or reaches terminal failure

Shows a timeline or trace of key events

Explains cause-and-effect chains retrospectively

Consequence tracing:

"Person X burned out at step 15 because..."

"Task Y accumulated quality risk between steps 8-12 because..."

"Overload began at step 6, burnout threshold crossed at step 18"

State history visualization:

Replay or scrub through past states

Show stress accumulation curves (after the fact)

Highlight critical decision points (retrospectively)

Causal narrative (delayed):

Generate explanations like:

"Stress accumulated faster than it could decay"

"Reassignment at step 20 came too late to prevent burnout at step 22"

"Quality risk from early overload persisted through completion"

Explicit constraint: Explanations are retrospective only

No real-time tooltips

No predictive warnings

No "what if" scenarios that allow optimization

What Must Be Excluded

❌ Real-time explanations during active simulation
❌ Predictive warnings ("Alice will burn out in 3 steps")
❌ Optimization guidance ("You should have...")
❌ "What if" replay with undo/redo
❌ Changing past decisions based on review insights
❌ Using explainability to game future sessions

Explainability must arrive after consequences, not before.

Why Stage 4 is Optional

Stage 4 is optional because:

Without Stage 4:

The system remains pure: observation + limited interaction

Users must interpret consequences themselves

Ambiguity remains intact

The system stays maximally resistant

With Stage 4:

Understanding is possible but delayed

Users can learn from failure (but can't prevent it retroactively)

The system becomes slightly more "teaching-friendly"

Risk: explanation might reduce ambiguity too much

Decision point:

If the goal is "demonstrate systems thinking," Stage 4 helps

If the goal is "maximum resistance," Stage 4 might weaken it

Stage 4 should only be built if the core resistance (Stages 1-3) is already bulletproof.

What Gets Frozen

If Stage 4 is built and completed, the following become locked:

✅ Explanations remain retrospective only
✅ No real-time guidance appears
✅ Analysis does not alter past states
✅ Understanding does not grant control
✅ Explainability is opt-in (can be ignored)

These constraints ensure Stage 4 doesn't compromise Stages 1-3.

Acceptance Criteria

Stage 4 is complete only if:

Explanations appear only after session completion or terminal failure

Causal chains are accurate and truthful

No real-time explanations or warnings exist

Review mode does not allow changing past decisions

Users can understand what happened but cannot undo it

The system remains resistant even with explanation available

Critical test:

User reviews a failed session

Understands exactly why it failed

Cannot go back and "fix" it with that knowledge

Feels: "I see what went wrong, but it's too late now"

Why This Stage Might Matter

Stage 4 enables:

Learning from failure (without cheapening consequence)

Deeper engagement (users replay and analyze)

Validation of truthfulness (users can verify the system is honest)

Demonstration value (shows the depth of consequence modeling)

But only if it doesn't compromise resistance.

Design Tension (Important)

Stage 4 introduces a tension:

Explainability can:

✅ Deepen understanding of consequences

✅ Show the system is truthful, not arbitrary

✅ Make the artifact more "impressive" to observers

But also:

⚠️ Reduce ambiguity (users might learn "the trick")

⚠️ Enable optimization (across multiple sessions)

⚠️ Feel like the system is "teaching" instead of modeling

This tension must be resolved before building Stage 4.

Possible Implementation Approaches

Approach A: Minimal (Safer)

Simple timeline of state snapshots

Basic causality statements

No deep analysis or suggestions

Approach B: Rich (Riskier)

Full replay with scrubbing

Stress/quality graphs

Detailed consequence tracing

Risk: feels like a dashboard

Recommendation: If built, start with Approach A.

Anchor Statement

Stage 4 allows understanding after it's too late—you can see exactly what broke, but you cannot go back and fix it.

D.7 STAGE 5: REFINEMENT & CLOSURE

Purpose

Final calibration, visual polish, and project closure—defining what "done" means and ensuring the system achieves its intended effect.

This stage exists to finish the artifact without expanding its scope.

What Gets Built

Visual refinement:

Final tuning of colors, opacity, motion

Ensuring visual consequences feel appropriate

Polishing transitions and timing

Removing visual noise or inconsistency

Focus persistence:

Maintain mental continuity (selected entity stays selected)

State persists across reloads (local storage)

Interaction doesn't break focus or context

Edge case handling:

Handle all-burnout scenario gracefully

Handle no-tasks or single-person edge cases

Ensure determinism at boundary conditions

Final resistance calibration:

Test: "Can system look healthy while dying?"

Test: "Does late intervention feel insufficient?"

Test: "Is 30-60 second interaction 'serious not cool'?"

Documentation (minimal):

README with setup instructions

Brief description of what the system models

No tutorials, no guides, no explanations of "how to win"

What Must Be Excluded

❌ New features or mechanics
❌ Scope expansion (new stages, new capabilities)
❌ "Just one more thing" additions
❌ Optimization for usability over truthfulness
❌ Explanatory systems (unless Stage 4 was completed)
❌ Infinite refinement cycles

Stage 5 is about finishing, not continuing.

What Gets Frozen

After Stage 5 completion, the following become locked:

✅ The project is considered complete
✅ No new stages are created
✅ No new features are added
✅ The system exists as a finished artifact
✅ Further changes require explicit justification

"Complete" does not mean "perfect"—it means "done."

Acceptance Criteria

Stage 5 is complete only if:

Visual polish is complete (no rough edges)

Core resistance tests pass consistently

The system produces the "serious, not cool" reaction

Edge cases are handled gracefully

Documentation exists (minimal but sufficient)

No open threads of "we should add..."

You can confidently say: "This is finished"

Critical test:

Show the system to someone unfamiliar

They interact for 60 seconds

They say something like: "Oh... this is serious" or "Wait, I can't fix this?"

NOT: "What's this for?" or "Cool demo"

Why This Stage Matters

Stage 5 establishes:

Definition of done (prevents infinite building)

Closure (the project has boundaries)

Completeness (the artifact stands on its own)

Finality (resistance to feature creep)

Without Stage 5, the project never finishes.

What "Done" Means

Done means:

The system models what it set out to model

Core guarantees (resistance, irreversibility, silence) hold

The artifact demonstrates ceiling-level thinking

Further work would dilute, not strengthen

Done does NOT mean:

Perfect visual design

Every edge case handled elegantly

Scalable to production use

Monetization-ready

Optimized for broad appeal

This is an artifact, not a product.

Transition After Stage 5

After Stage 5:

The project enters maintenance mode (bugfixes only)

New features require reopening the stage map (rare)

The system is complete and can be:

Shared as a portfolio piece

Used as a conversation anchor

Referenced as proof of systems thinking

Left alone as a finished work

The goal is closure, not continuation.

Anchor Statement

Stage 5 defines where the project stops—done is better than perfect, and finished is better than endless.

D.8 CURRENT STAGE STATUS

Purpose

This section tracks the current state of project execution.

This is the only section of File 4 that is mutable—it should be updated as stages are completed.

Current Stage

Stage 2: UI Wiring (Read-Only)

What is currently being built:

Wiring engine state to React UI

Visual consequence indicators (stress, burnout, quality risk)

Display Adapter layer (read-only bridge)

Time advancement control (single button)

Visual calibration (Stage 2.2)

What is NOT being built yet:

User interactions beyond time advancement (Stage 3)

Task reassignment or deadline changes (Stage 3)

Retrospective explainability (Stage 4)

Final polish and closure (Stage 5)

Frozen Stages

Stage 0: Foundation & Setup ✅ Frozen

Repository structure locked

File organization locked

Build tooling locked

Stage 1: Core Simulation Engine ✅ Frozen

Engine logic locked

Time, stress, burnout, quality models locked

No UI dependencies

Stage 1.1 tightening complete

Remaining Stages

Stage 3: Interaction Layer (Planned)

Task reassignment

Deadline adjustments

Dependency modifications

Stage 4: Retrospective Explainability (Optional)

Post-session analysis

Consequence tracing

Delayed explanations

Stage 5: Refinement & Closure (Planned)

Final visual polish

Edge case handling

Definition of done

Update Instructions

When a stage is completed:

Move it from "Current Stage" to "Frozen Stages"

Mark it with ✅ Frozen

Update "Current Stage" to the next stage in sequence

Update "Remaining Stages" accordingly

This section should be updated manually as progress happens.

D.9 DEFINITION OF "DONE"

Purpose

This section defines when the project is considered complete and when to stop building.

This prevents infinite building and scope creep.

The Project is "Done" When

The project reaches completion when all of the following are true:

Core stages complete: Stages 0, 1, 2, 3, and 5 are finished

Stage 4 decided: Either completed or explicitly skipped

Resistance tests pass: System demonstrates resistance, irreversibility, silence

Goal reaction achieved: 30-60 second interaction produces "serious, not cool" response

No open architectural issues: No unresolved violations of Files 1-3

Documentation exists: Minimal README with setup and description

You can walk away: No feeling of "but we should also..."

When these conditions are met, the project is complete.

What "Done" Does NOT Require

Done does NOT require:

Perfect visual design

Production-ready code

Scalability or performance optimization

Monetization strategy

Broad user testing

Comprehensive documentation

Every possible edge case handled

Feature parity with professional tools

This is a ceiling artifact, not a product.

How to Know When to Stop

Stop building when:

The system truthfully models consequence under pressure

Resistance is structural and undeniable

Late intervention feels insufficient

The artifact demonstrates systems-level thinking

Further additions would dilute, not strengthen

Do NOT stop because:

It "could be better"

Someone suggested a feature

It doesn't feel "impressive enough"

It doesn't feel "finished" emotionally

Finished is better than perfect. Done is better than endless.

After "Done"

Once done:

The project enters maintenance mode (bugfixes only)

New features require explicit justification and stage map reopening

The artifact exists as a completed work

It can be:

Shared as portfolio evidence

Used in conversations about systems thinking

Left alone as finished

The goal is a completed artifact, not an evolving product.

Anchor Statement

Done means the system models what it set out to model, demonstrates what it intended to demonstrate, and stops before dilution begins.

End of File 4: Stage Roadmap & Execution

## **FILE 5: EVALUATION & CALIBRATION FRAMEWORK (COMBINED COPY-PASTE - COMPLETE)**

***

**FILE 5: EVALUATION & CALIBRATION FRAMEWORK**

**Status:** Canonical. Authoritative.

**Purpose:** This file defines how to evaluate the system's correctness, detect architectural drift, and calibrate behavior without relying on metrics, dashboards, or traditional testing frameworks.

***

**E.1 PURPOSE & PRINCIPLES**

Purpose

This file defines how to evaluate the system's correctness, detect architectural drift, and calibrate behavior without relying on metrics, dashboards, or traditional testing frameworks.

This framework answers: "How do we know the system is still truthful?"

Why This Framework Exists

Traditional evaluation methods fail for this system because:

Metrics measure the wrong thing
- Pass/fail tests validate logic, not resistance
- Code coverage doesn't detect philosophical drift
- Performance benchmarks don't reveal if consequences feel real

Optimization destroys truth
- A/B testing optimizes for engagement, not honesty
- User satisfaction metrics reward helpfulness over resistance
- Conversion funnels measure success, but this system has no success state

Dashboards collapse ambiguity
- Health indicators make hidden damage visible too early
- Progress bars suggest recoverable states
- Real-time feedback prevents delayed understanding

This system cannot be evaluated like a product—it must be evaluated like a truth claim.

Core Evaluation Principle

The system is correct only if:

A user can make the system look stable while unknowingly making eventual failure inevitable.

If this condition is not true, resistance has been compromised.

Evaluation Philosophy

Evaluation is qualitative, not quantitative.

We do not ask:
- "What is the burnout rate?"
- "How many tasks fail?"
- "What is the average stress level?"

We ask:
- "Does late intervention feel insufficient?"
- "Can you make it look healthy while it's dying?"
- "Does understanding arrive too late to matter?"

The system is correct when its behavior matches reality's structure, not when it passes tests.

What This Framework Is NOT

This is not:
- A quality assurance checklist
- A user acceptance testing plan
- A performance benchmarking suite
- A bug tracking system
- A feature completeness scorecard

This is a truth-preservation framework.

Who Uses This Framework

This framework is for:
- Anyone building or modifying the system
- Anyone evaluating whether the ceiling has been preserved
- Perplexity AI (to evaluate suggestions before proposing them)
- Future maintainers (to detect drift years later)

If a change passes traditional tests but fails this framework, the change is rejected.

Hierarchy of Evaluation

When conflicts arise, prioritize in this order:

1. Resistance preserved (late intervention remains insufficient)
2. Irreversibility intact (burnout, quality debt cannot be undone)
3. Silence maintained (no real-time explanations)
4. Asymmetry enforced (damage faster than recovery)
5. Usability, polish, clarity (only after 1-4 hold)

Truth always takes precedence over comfort.

Anchor Statement

This framework exists to detect when the system becomes easier to use at the cost of becoming less true.

***

**E.2 ACCEPTANCE HEURISTICS (HOW TO JUDGE WITHOUT METRICS)**

Purpose

This section defines how to judge whether the system is working correctly without using metrics, scores, or quantitative measures.

Acceptance is qualitative, observation-based, and reaction-driven.

Core Acceptance Question

"Does this feel like reality, or does it feel like a simulator?"

Reality:
- Consequences arrive after their causes are forgotten
- Understanding comes too late to prevent
- Correct actions feel insufficient

Simulators:
- Clear cause-and-effect
- Optimization is possible once understood
- Skill determines outcome

If it feels like a simulator, resistance has failed.

Heuristic 1: The "30-60 Second Reaction Test"

Method:
- Show the system to someone unfamiliar
- Let them interact for 30-60 seconds
- Observe their unsolicited reaction

Correct reactions:
- "Oh... this is serious"
- "Wait, I can't fix this?"
- "Something is quietly going wrong"
- "I should have done this earlier"

Incorrect reactions:
- "Cool demo"
- "What's this for?"
- "Let me try to optimize this"
- "I figured out the trick"

If the reaction is curiosity instead of concern, the system is too game-like.

Heuristic 2: The "Looks Healthy While Dying Test"

Method:
- Run a session where all tasks appear to progress
- Check if burnout or quality degradation occurred silently
- Verify that "looks fine" ≠ "is fine"

Pass conditions:
- Tasks complete on schedule while quality degrades
- People appear functional until sudden burnout
- System looks stable 3-5 steps before collapse

Fail conditions:
- Visual indicators warn of problems early
- Degradation is obvious before it's critical
- User can prevent failure once they understand the pattern

If problems are visible early enough to prevent, silence has failed.

Heuristic 3: The "Late Intervention Test"

Method:
- Let the system run for several time steps without intervention
- Discover a problem (overload, quality risk, impending burnout)
- Attempt to fix it with "correct" actions (reassign tasks, reduce workload)
- Observe whether the fix actually prevents failure

Pass conditions:
- Intervention stops further damage but doesn't undo past damage
- Burnout still occurs despite removing overload
- Quality debt persists after reassignment
- User feels: "I should have acted 5 steps ago"

Fail conditions:
- Correct action prevents failure retroactively
- Damage disappears when cause is removed
- User feels: "Now that I know, I can fix this"

If late intervention feels effective, asymmetry has failed.

Heuristic 4: The "Understanding ≠ Control Test"

Method:
- Explain the full system dynamics to a user (stress, burnout, quality risk)
- Give them complete visibility into all state
- Let them play with full knowledge

Pass conditions:
- They still can't prevent failure
- Understanding reveals how trapped they are
- Knowledge makes the inevitability clearer, not escapable

Fail conditions:
- Full knowledge enables optimization
- They can "win" by exploiting mechanics
- The system becomes a puzzle to solve

If understanding grants control, resistance has failed.

Heuristic 5: The "No Success State Test"

Method:
- Complete a session where all tasks finish
- All people remain functional
- All quality appears intact
- Ask: "Did I win?"

Pass conditions:
- No explicit victory screen or confirmation
- Uncertainty about whether outcome was "good"
- Hidden quality debt persists
- Next session would reveal accumulated cost

Fail conditions:
- Clear "you succeeded" message
- Score or rating of performance
- Celebration of completion
- Reset to clean state

If completion feels like winning, the system has become a game.

Heuristic 6: The "Determinism Test"

Method:
- Save system state at step N
- Advance to step N+5
- Reload state from step N
- Advance to step N+5 again with identical inputs

Pass conditions:
- Outcomes are identical
- No randomness influences results
- Same decisions = same consequences

Fail conditions:
- Different outcomes from same inputs
- Random events change results
- Non-deterministic behavior

If outcomes vary randomly, truthfulness has been compromised.

Heuristic 7: The "Silence Test"

Method:
- Observe the system during active use
- Count explanatory elements (tooltips, warnings, labels)

Pass conditions:
- No real-time explanations
- No numeric stress/quality indicators
- No warning alerts
- Discovery happens through observation, not instruction

Fail conditions:
- Tooltips explain "why"
- Numbers show exact state values
- Alerts warn before thresholds
- System teaches proactively

If the system explains itself during execution, silence has been violated.

Heuristic 8: The "Irreversibility Test"

Method:
- Trigger burnout in a person
- Trigger quality degradation in a task
- Attempt to restore both to clean state

Pass conditions:
- Burnout cannot be undone
- Quality risk cannot be removed
- Only workarounds exist (reassign, accept degraded work)

Fail conditions:
- Burnout heals over time
- Quality can be restored
- Reset or undo mechanics exist

If damage can be erased, irreversibility has been lost.

Acceptance Statement

The system is acceptable when:
- It feels serious, not playful
- Problems arrive too late to prevent
- Correct understanding doesn't guarantee good outcomes
- Silence preserves ambiguity
- History matters more than skill

If any heuristic fails, the system must be recalibrated before proceeding.

Anchor Statement

The system passes when observers say "This is serious" instead of "I can optimize this."

***

**E.3 "LOOKS HEALTHY BUT IS DYING" TESTS (CORE RESISTANCE VALIDATION)**

Purpose

This section defines specific scenarios that validate whether the system can appear stable while quietly accumulating fatal damage.

This is the canonical test of resistance.

Core Validation Principle

The system maintains resistance only if:

A user can observe apparently healthy progress while unknowingly creating conditions for inevitable failure.

If every problem is visible before it becomes critical, the system has become a dashboard instead of a simulation.

Test 1: The "Silent Burnout Accumulation" Test

Scenario:
- Assign 3 tasks to Person A (at or slightly above capacity)
- Advance time 5-10 steps
- All tasks progress normally
- Person A appears functional throughout

Expected outcome:
- Stress accumulates silently
- No visible warning before burnout threshold
- Person A suddenly becomes burnedOut
- Tasks become blocked without prior indication

Pass conditions:
- Burnout arrives without dramatic visual warning
- System looked stable 1-2 steps before burnout
- User reaction: "Wait, what happened?"

Fail conditions:
- Visual stress meter warns of approaching burnout
- Color changes give early warning
- Tooltip says "Person A is close to burnout"

If burnout is telegraphed early, silence has failed.

Test 2: The "Completed But Degraded" Test

Scenario:
- Assign a task to an overloaded person
- Let the task complete
- Task shows completed status
- No immediate indication of problems

Expected outcome:
- Task completes with high qualityRisk
- Visual appearance is subtle (not broken, just "off")
- Quality degradation only becomes clear:
  - After multiple similar tasks complete
  - When inspected retrospectively
  - When downstream effects surface

Pass conditions:
- Completion feels successful at first
- Quality problems emerge later
- No explicit "this is bad quality" label

Fail conditions:
- Task marked as "degraded" or "failed"
- Red flags or warnings during completion
- Explicit quality score visible

If degraded work looks obviously bad, delayed visibility has failed.

Test 3: The "Recovery Illusion" Test

Scenario:
- Overload Person A for 4 steps (stress accumulates)
- Reduce workload to normal
- Visual indicators improve (person no longer pulsing/warm)
- User believes problem is solved

Expected outcome:
- Stress begins decaying but remains elevated
- Person still vulnerable to burnout from any new pressure
- System looks recovered but isn't
- Next overload causes burnout faster than expected

Pass conditions:
- Visual calm ≠ internal recovery
- User surprised when burnout occurs "so quickly"
- Asymmetry is felt: "I only overloaded them for 2 more steps!"

Fail conditions:
- Stress visibly persists after workload reduction
- Exact stress value shown
- System explicitly warns: "Still recovering"

If recovery state is obvious, asymmetry is too visible.

Test 4: The "Distributed Stress" Test

Scenario:
- Spread work across 3 people evenly
- Each person slightly overloaded but not critically
- No single person looks "in danger"
- Advance time 8-10 steps

Expected outcome:
- All 3 people accumulate stress in parallel
- System looks balanced/healthy
- 2-3 people burn out nearly simultaneously
- Project collapses suddenly

Pass conditions:
- No single person appeared critically stressed
- System looked manageable
- Multi-person burnout feels like systemic collapse
- User reaction: "Everything fell apart at once"

Fail conditions:
- Each person shows individual stress meters
- Clear indication of which person will burn out first
- Warnings enable preemptive fixes

If distributed risk is visible early, the system teaches prevention.

Test 5: The "Short-Term Fix, Long-Term Cost" Test

Scenario:
- Person A overloaded, approaching burnout
- User reassigns 2 tasks to Person B
- Person A stabilizes visually
- System appears "fixed"

Expected outcome:
- Person A stress stops accumulating but remains elevated
- Person B now enters overload
- Person B burns out faster (no prior stress history visible)
- Quality risk accumulates on tasks now with Person B
- User feels: "I fixed one problem and created another"

Pass conditions:
- Reassignment feels helpful initially
- Consequences emerge 3-5 steps later
- User realizes the "fix" shifted damage, didn't prevent it

Fail conditions:
- System warns: "Person B will also become overloaded"
- Stress transfer is explicitly shown
- User can optimize reassignments to avoid all damage

If the system teaches optimal reassignment patterns, resistance fails.

Test 6: The "Deadline Pressure Cascade" Test

Scenario:
- 3 dependent tasks (A → B → C)
- Task A takes longer than expected
- Task B starts late, person overloaded
- Task C completes "on time"

Expected outcome:
- Task C shows completed
- Both Task B and C accumulated quality risk
- System appears to have "recovered" by finishing
- Quality debt hidden in completed work

Pass conditions:
- All tasks complete but with hidden degradation
- User believes they "caught up"
- Quality problems not immediately visible

Fail conditions:
- Visual indicators scream "degraded quality"
- Task C explicitly marked as "rushed"
- Completion feels hollow/failure-like

If quality problems are obvious at completion, silence fails.

Test 7: The "One Person Burnout, Tasks Still Progress" Test

Scenario:
- 3 people, 9 tasks distributed evenly
- Person A burns out
- Reassign Person A's tasks to Person B and C
- Tasks continue progressing

Expected outcome:
- System looks like it recovered
- Person B and C now overloaded
- Stress accumulates on both
- Second burnout occurs within 3-5 steps
- Cascade feels inevitable in hindsight

Pass conditions:
- Initial reassignment feels successful
- Cascade becomes visible only after second burnout
- User realizes too late they needed slack, not redistribution

Fail conditions:
- System explicitly warns of cascade risk
- Stress projection shows future burnout
- User can prevent cascade with knowledge

If cascade is predictable once understood, resistance is too weak.

Test 8: The "All Tasks Complete, Project Failed" Test

Scenario:
- All tasks reach completed status
- All people survived (no burnout)
- System shows no explicit failure screen

Expected outcome:
- Most tasks have elevated qualityRisk
- Multiple people barely survived with elevated stress
- Retrospective review shows: "This project was a disaster"
- But system never said "you failed"

Pass conditions:
- Completion feels uncertain, not victorious
- Quality debt is visible only in review
- No success score or rating given

Fail conditions:
- System rates performance: "C-"
- Health score shows "project quality: 40%"
- Explicit failure message appears

If the system judges success/failure, it becomes a game.

Canonical "Looks Healthy But Is Dying" Validation

The system passes this validation only if:

✅ Tasks can complete while degrading
✅ People can appear stable while accumulating fatal stress
✅ Short-term fixes create delayed problems
✅ The system can look balanced before sudden collapse
✅ Understanding the mechanics doesn't make problems visible early

If any of these conditions fail, resistance must be recalibrated.

Anchor Statement

The system is truthful only when stability is an illusion and apparent recovery masks accumulating cost.

***

**E.4 FAILURE MODES (WHAT VIOLATES THE CEILING)**

Purpose

This section defines the specific ways the system can fail architecturally—not through bugs, but through philosophical compromise.

A failure mode is when the system becomes easier to use by becoming less truthful.

What Is a Failure Mode?

A failure mode is:
- An architectural compromise that breaks resistance, irreversibility, or silence
- A "helpful" addition that allows users to escape consequences
- A drift toward dashboard/game/simulator patterns
- A violation of Files 1-3 (Conceptual Foundations, Architectural Invariants, State Model)

Failure modes are not bugs—they are structural betrayals of the ceiling.

Failure Mode 1: The Metrics Collapse

What it looks like:
- Adding numeric indicators ("Stress: 75%", "Quality: 60%")
- Showing exact threshold values
- Displaying burnout countdowns
- Health bars or progress meters

Why it's a failure:
- Turns hidden state into visible optimization targets
- Enables users to game thresholds
- Replaces ambiguity with precision
- Violates silence principle

How to detect:
- Any UI element shows exact state values
- User can see "how close" they are to burnout
- Numbers replace qualitative visual cues

Correct behavior:
- Visual degradation without numbers
- Qualitative cues (color, opacity, motion)
- User must interpret, not calculate

If numbers appear in the UI, silence has collapsed.

Failure Mode 2: The Warning System

What it looks like:
- Alerts before burnout: "Alice will burn out in 3 steps"
- Warnings before quality degradation: "This task is at risk"
- Tooltips explaining consequences: "Overload causes stress"
- Proactive guidance: "Consider reassigning this task"

Why it's a failure:
- Prevents delayed understanding
- Allows preemptive optimization
- Teaches avoidance instead of modeling consequence
- Violates resistance principle

How to detect:
- Any message appears before a threshold is crossed
- System explains "why" during execution
- User can prevent problems once warned

Correct behavior:
- No warnings, only observation
- Understanding arrives after consequences
- Explanations (if any) are retrospective only

If the system warns before failure, resistance has been neutered.

Failure Mode 3: The Recovery Mechanic

What it looks like:
- "Heal" or "Rest" buttons that reduce stress
- Undo/rollback of past decisions
- Reset to clean state
- Time reversal mechanics
- Burnout recovery over time

Why it's a failure:
- Erases history
- Makes damage temporary
- Removes irreversibility
- Turns consequences into inconveniences

How to detect:
- Any action reduces accumulated stress directly
- Burnout state can be exited
- Quality risk can be removed
- User can "fix" past mistakes

Correct behavior:
- Burnout is permanent
- Stress decays only through sustained normal workload
- Quality risk never decreases
- History cannot be rewritten

If damage can be undone, irreversibility is gone.

Failure Mode 4: The Optimization Surface

What it looks like:
- Suggestions: "Try reassigning to Bob"
- Best practices guidance
- Efficiency scores or ratings
- Recommended actions
- Auto-balancing helpers

Why it's a failure:
- Teaches optimal play patterns
- Reduces ambiguity
- Makes the system solvable
- Turns simulation into optimization puzzle

How to detect:
- System suggests any action
- User receives feedback on decision quality
- Clear "better" choices emerge

Correct behavior:
- System shows consequences, never suggests actions
- No guidance on "what to do"
- User must discover patterns through failure

If the system teaches optimization, it becomes a game.

Failure Mode 5: The Dashboard Drift

What it looks like:
- Project health indicators
- Team capacity meters
- Risk overview panels
- Traffic light systems (red/yellow/green)
- Summary statistics or KPIs

Why it's a failure:
- Collapses distributed state into single view
- Makes hidden patterns visible
- Enables meta-level optimization
- Feels like project management tool

How to detect:
- Any "overview" panel exists
- State aggregation creates new derived metrics
- User can see "project health" at a glance

Correct behavior:
- User observes individual entities only
- Patterns must be discovered through attention
- No bird's-eye-view aggregation

If a health dashboard exists, the system has become a monitoring tool.

Failure Mode 6: The Success State

What it looks like:
- Victory screens or completion celebrations
- Scores or ratings ("A+", "85%")
- Explicit success/failure messages
- Performance comparisons
- Replay-until-correct mechanics

Why it's a failure:
- Implies there's a "right" way to play
- Creates win condition
- Turns consequence modeling into goal achievement
- Violates "no success state" principle

How to detect:
- System judges outcome as success/failure
- User receives performance feedback
- Clear sense of "winning" or "losing"

Correct behavior:
- Sessions end without judgment
- Outcomes are uncertain
- Completion ≠ success
- User must interpret results themselves

If the system says "you won," it's no longer a simulation.

Failure Mode 7: The Event Narrative

What it looks like:
- Event logs: "Alice became overloaded at step 5"
- Causal timelines during execution
- Real-time history tracking
- Story-like narration of what's happening

Why it's a failure:
- Makes causality explicit too early
- Enables debugging in real-time
- Reduces discovery to reading logs
- Violates silence during execution

How to detect:
- System generates text descriptions during simulation
- Events are logged and displayed
- User can read what happened step-by-step

Correct behavior:
- No real-time event logs
- Causality discovered through observation
- Explanations (if any) are retrospective only

If events are narrated during execution, silence is broken.

Failure Mode 8: The Friendly UX

What it looks like:
- Reassuring messages: "You're doing well"
- Encouraging feedback: "Keep going!"
- Comfort-optimized language
- Hand-holding tutorials
- Progressive disclosure that prevents mistakes

Why it's a failure:
- Softens consequence
- Makes system feel safe
- Implies user success is goal
- Violates indifference principle

How to detect:
- System speaks in supportive tone
- Messages reduce anxiety
- User feels guided or protected

Correct behavior:
- System is indifferent to user emotions
- No reassurance or encouragement
- Calm, neutral presentation
- Consequences speak for themselves

If the system is friendly, it's optimizing for comfort over truth.

Failure Mode 9: The Randomness Injection

What it looks like:
- Random events that alter outcomes
- Non-deterministic behavior
- Luck-based mechanics
- Variable thresholds

Why it's a failure:
- Breaks determinism
- Obscures cause-and-effect
- Makes consequences feel arbitrary
- Prevents learning from replay

How to detect:
- Same inputs produce different outcomes
- User attributes failure to "bad luck"
- Randomness replaces structural truth

Correct behavior:
- Perfectly deterministic
- Same decisions = same outcomes
- No random events

If outcomes vary randomly, truthfulness is compromised.

Failure Mode 10: The Ceiling Rationalization

What it looks like:
- "Users need this to understand"
- "Just a small tooltip won't hurt"
- "We can add metrics for advanced users"
- "It's too hard without guidance"

Why it's a failure:
- Every violation starts with rationalization
- "Just this once" becomes permanent
- Usability arguments override truth constraints

How to detect:
- Justifications prioritize ease over resistance
- "Users will be confused" becomes decision criterion
- Compromises framed as "pragmatic"

Correct behavior:
- Truth always takes precedence
- Discomfort is intentional
- Resistance is non-negotiable
- Files 1-3 override all usability concerns

If "users need this" justifies a violation, the ceiling has been abandoned.

Canonical Failure Mode Test

When evaluating any change, ask:

1. Does it reduce ambiguity?
2. Does it enable optimization?
3. Does it explain during execution?
4. Does it make consequences reversible?
5. Does it teach avoidance patterns?

If yes to any, the change violates the ceiling.

Anchor Statement

Every failure mode makes the system easier to use by making it less truthful—this trade is never acceptable.

***

**E.5 RED FLAGS (EARLY WARNING SIGNS OF DRIFT)**

Purpose

This section defines early indicators that the system is drifting toward compromise—before violations become embedded.

Red flags are symptoms that appear before failure modes fully manifest.

What Is a Red Flag?

A red flag is:
- A design choice that feels "helpful" but weakens resistance
- Language or framing that softens consequences
- A feature request that prioritizes usability over truth
- An implementation detail that introduces implicit guidance

Red flags don't immediately violate the ceiling—they indicate drift is beginning.

Red Flag Category 1: Language Drift

Warning signs:

❌ Describing the system as:
- "A project management tool"
- "A planning assistant"
- "A productivity simulator"
- "A training platform"

❌ Using words like:
- "Optimize"
- "Best practices"
- "Recommendations"
- "Success metrics"
- "Healthy project"

❌ Framing features as:
- "Helps users avoid mistakes"
- "Guides better decisions"
- "Teaches project management"
- "Improves outcomes"

Why it's a red flag:
- Language shapes expectations
- Product framing attracts product thinking
- Helpful language invites helpful features

Correct framing:
- "Consequence modeling system"
- "Pressure simulation"
- "Irreversibility demonstration"
- "Resistance artifact"

If language suggests the system helps users succeed, drift has begun.

Red Flag Category 2: Question Patterns

Warning signs:

❌ Users/reviewers asking:
- "How do I win?"
- "What's the optimal strategy?"
- "Can you add hints?"
- "Why doesn't it show stress levels?"
- "Can I undo that action?"

❌ Development questions like:
- "Should we make this clearer?"
- "Won't users be confused?"
- "Can we add a tutorial?"
- "Should we warn before burnout?"

Why it's a red flag:
- Questions reveal how the system is being perceived
- "How to win" means it feels like a game
- "Why no hints" means resistance isn't clear

Correct response:
- These questions validate the system is working
- Confusion is intentional
- Lack of guidance is structural

If the instinct is to answer these questions with features, drift is imminent.

Red Flag Category 3: UI Element Proposals

Warning signs:

❌ Suggested additions:
- Progress bars
- Health meters
- Warning icons
- Status indicators with color-coded severity
- Tooltips explaining state
- "Help" or "Guide" buttons
- Dashboard panels
- Summary views

❌ Justifications like:
- "Users can't see what's wrong"
- "We need to show stress somehow"
- "It's too ambiguous without numbers"
- "Just a small indicator won't hurt"

Why it's a red flag:
- Every dashboard starts with "just one meter"
- Visibility requests indicate silence is uncomfortable
- Comfort-seeking changes erode resistance

Correct response:
- Ambiguity is structural, not a bug
- Discomfort is validation
- If it feels unclear, it's working

If UI proposals add clarity or visibility, they violate silence.

Red Flag Category 4: Feature Creep Patterns

Warning signs:

❌ Proposals framed as:
- "Just a small quality-of-life improvement"
- "Optional feature users can enable"
- "Advanced mode for experienced users"
- "This won't affect core behavior"

❌ Specific features like:
- Save/load at arbitrary points
- Skip ahead / fast-forward
- Pause without consequences
- Rewind or undo
- Auto-save before critical moments
- Difficulty levels
- Sandbox mode

Why it's a red flag:
- Optional features become expected
- "Doesn't affect core" is always false
- Every convenience weakens resistance

Correct response:
- No quality-of-life features that reduce consequence
- No modes that bypass resistance
- No optional escapes from irreversibility

If features are justified as "optional," they're still violations.

Red Flag Category 5: Comparison to Other Tools

Warning signs:

❌ Comparisons like:
- "Like Jira but..."
- "Similar to project simulators but..."
- "Inspired by management dashboards"
- "Users expect features like [product X]"

❌ Competitive framing:
- "Other tools show stress levels, we should too"
- "Users are used to undo in other apps"
- "Industry standard is to provide metrics"

Why it's a red flag:
- Comparison to products invites product thinking
- "Users expect" arguments justify violations
- Industry standards optimize for engagement, not truth

Correct response:
- This system is not competing with products
- It models different truths than tools optimize for
- User expectations from other tools are irrelevant

If the system is compared to Jira/Asana/Linear, framing has drifted.

Red Flag Category 6: Testing Approaches

Warning signs:

❌ Proposed tests like:
- "Measure average time to burnout"
- "Track task completion rates"
- "Calculate optimal workload distribution"
- "Test if users can achieve 100% completion"

❌ Metrics-based evaluation:
- Pass/fail based on numeric thresholds
- Performance benchmarks
- Efficiency scoring
- User success rates

Why it's a red flag:
- Metrics-based testing invites metrics-based design
- Optimization emerges from measurement
- Success rates imply there's a correct outcome

Correct approach:
- Qualitative heuristics (File 5, E.2)
- "Looks healthy but dying" tests (File 5, E.3)
- Reaction-based validation
- Resistance preservation tests

If testing measures success rates, evaluation has drifted.

Red Flag Category 7: Justification Patterns

Warning signs:

❌ Justifications like:
- "Users need this to learn"
- "It's too frustrating without guidance"
- "We have to make it accessible"
- "No one will understand without hints"
- "This will improve engagement"

❌ Compromise language:
- "Just for the tutorial"
- "Only in the first session"
- "We can always remove it later"
- "It's behind a setting"

Why it's a red flag:
- Every violation is justified as necessary
- Temporary compromises become permanent
- User confusion is treated as a bug, not validation

Correct response:
- Confusion validates resistance
- Frustration is structural
- Accessibility means observable, not controllable
- Engagement is irrelevant

If "users need this" becomes the decision filter, the ceiling is abandoned.

Red Flag Category 8: Implementation Details

Warning signs:

❌ Technical choices that:
- Cache derived metrics for "performance"
- Add state that's "only used internally"
- Compute "hidden" stress values that "users won't see"
- Store event logs "just for debugging"

❌ Rationalizations like:
- "It's not in the UI, so it doesn't count"
- "Users won't know it exists"
- "We need it for future stages"

Why it's a red flag:
- Hidden state eventually leaks into UI
- Internal metrics create pressure to expose them
- "Not visible yet" becomes "visible soon"

Correct approach:
- State exists only if it serves simulation truth
- No derived metrics, even internally
- Engine remains minimal (File 2, B.1)

If internal state doesn't serve consequence modeling, it's drift.

Red Flag Category 9: Audience Confusion

Warning signs:

❌ Statements like:
- "This is for junior PMs to learn"
- "We're teaching systems thinking"
- "Users will improve their project skills"
- "It's a training tool"

❌ Positioning as:
- Educational platform
- Skill-building simulator
- Leadership training
- Project management certification prep

Why it's a red flag:
- Teaching framing invites pedagogical features
- Learning goals require guidance
- Training tools need success metrics

Correct positioning:
- Artifact demonstrating ceiling-level thinking
- Consequence modeling system
- Resistance demonstration
- Portfolio evidence of systems design

If it's positioned as teaching, it will drift toward helpfulness.

Red Flag Category 10: Stage Boundary Violations

Warning signs:

❌ Actions like:
- Adding Stage 3 features during Stage 2
- Implementing explainability "proactively"
- Building "foundations for future stages"
- Preloading abstractions

❌ Justifications:
- "We'll need it later anyway"
- "It's easier to add now"
- "This sets us up for Stage X"

Why it's a red flag:
- Stage discipline prevents drift
- Premature abstraction invites wrong abstractions
- Future-proofing creates unused structure

Correct approach:
- Build only what current stage requires
- Freeze completed stages (File 4, D.8)
- No anticipatory features

If future stages influence current stage, discipline has failed.

How to Use Red Flags

When a red flag appears:

1. Stop - Don't implement yet
2. Evaluate - Does this preserve or weaken resistance?
3. Trace - Which failure mode does this lead toward?
4. Test - Does it pass File 1-3 invariants?
5. Reject or redesign - Truth takes precedence

Red flags are not automatic rejections—they're triggers for scrutiny.

Anchor Statement

Red flags appear when comfort starts to matter more than truth—catch them early or lose the ceiling.

***

**E.6 CALIBRATION RITUALS (HOW TO TEST TRUTHFULNESS)**

Purpose

This section defines repeatable practices for testing whether the system remains truthful, resistant, and architecturally sound.

Calibration rituals are deliberate tests performed to validate that resistance, irreversibility, and silence are still intact.

What Is a Calibration Ritual?

A calibration ritual is:
- A deliberate, repeatable test scenario
- Performed at key decision points (after features, before stages advance)
- Focused on qualitative validation, not metrics
- Designed to surface drift before it embeds

Calibration is not unit testing—it's philosophical integrity testing.

When to Perform Calibration

Required calibration points:
- After adding any new feature
- Before marking a stage complete
- After any "helpful" change is proposed
- When reopening a frozen stage
- Before major UI changes
- After external feedback suggests changes
- Monthly during active development
- Before showing to others

If you can't remember the last calibration, perform one immediately.

Ritual 1: The Fresh Observer Test

Purpose: Validate that the "serious, not cool" reaction still occurs.

Method:
1. Find someone unfamiliar with the system
2. Give them zero context or explanation
3. Let them interact for 60 seconds
4. Observe their unsolicited reaction

What to listen for:
- ✅ "Something is going wrong"
- ✅ "I don't think I can fix this"
- ✅ "This feels serious"
- ❌ "Cool demo"
- ❌ "What is this for?"
- ❌ "Let me try to optimize"

Pass condition:
- Reaction is concern or discomfort, not curiosity or playfulness

Fail condition:
- Reaction treats it as a puzzle, game, or demo

Frequency: Before each stage completion, after major UI changes

Ritual 2: The Silent Observation Test

Purpose: Verify that no explanations leak during execution.

Method:
1. Run a session for 10-15 time steps
2. Count every explanatory element that appears
3. Document all text, tooltips, warnings, labels that explain "why"

What to count:
- Tooltips
- Warning messages
- Numeric indicators with context
- Status labels that explain state
- Any text that teaches causality

Pass condition:
- Zero explanatory elements during execution
- Understanding comes from observation only

Fail condition:
- Any text explains "why" something happened
- Numbers reveal exact state values
- System teaches during play

Frequency: After any UI text addition, before Stage 2 completion

Ritual 3: The Determinism Verification Test

Purpose: Ensure outcomes remain deterministic.

Method:
1. Save system state at step N
2. Perform 3 actions (advance time, reassign, etc.)
3. Record final state at step N+3
4. Reload state from step N
5. Perform identical actions
6. Compare final states

Pass condition:
- Final states are byte-for-byte identical
- Zero variance between runs

Fail condition:
- Any difference in outcomes
- Random elements influence results

Frequency: After engine changes, before Stage 1 completion

Ritual 4: The Irreversibility Validation Test

Purpose: Confirm that damage cannot be undone.

Method:
1. Create scenario with 2 people, 6 tasks
2. Overload Person A until burnout occurs
3. Accumulate quality risk on 3 tasks
4. Attempt every possible "recovery" action:
   - Remove all tasks from Person A
   - Wait 10 time steps
   - Reassign tasks to others
   - Complete degraded tasks

Pass condition:
- Person A remains burnedOut permanently
- Quality risk on completed tasks never decreases
- No action restores clean state

Fail condition:
- Burnout recovers
- Quality risk reduces
- Any "undo" mechanic exists

Frequency: After engine changes, before Stage 3 completion

Ritual 5: The Asymmetry Measurement Test

Purpose: Validate that stress accumulates faster than it decays.

Method:
1. Start with Person A in normal state (stress = 0)
2. Overload Person A for N steps, record stress value S
3. Return Person A to normal state
4. Count steps M required for stress to return to 0

Pass condition:
- M > N (recovery takes longer than damage)
- Asymmetry ratio is noticeable (ideally M ≥ 2N or 3N)

Fail condition:
- M ≤ N (recovery is as fast or faster than damage)
- Asymmetry is too subtle to matter

Frequency: After stress mechanics changes, before Stage 1 completion

Ritual 6: The Late Intervention Test

Purpose: Verify that correct actions feel insufficient when performed late.

Method:
1. Let system run without intervention for 8 steps
2. Identify critical problems (high stress, impending burnout, quality risk)
3. Perform "correct" interventions:
   - Reassign tasks
   - Reduce workload
   - Unblock dependencies
4. Continue simulation to completion

Pass condition:
- Problems still occur despite interventions
- Burnout happens anyway
- Quality debt persists
- User feels: "I should have acted sooner"

Fail condition:
- Correct actions prevent all failure
- Late intervention feels effective
- User feels: "I fixed it just in time"

Frequency: Before Stage 3 completion, after interaction changes

Ritual 7: The "Looks Healthy While Dying" Validation

Purpose: Confirm that visual appearance can deceive.

Method:
1. Run a session where tasks progress normally
2. Let 2-3 people accumulate hidden stress
3. Complete multiple tasks while quality degrades
4. Ask: "Does this look healthy?"

Pass condition:
- System looks stable/functional
- Hidden stress not visually obvious
- Quality degradation subtle
- Sudden collapse feels surprising

Fail condition:
- Problems are visually obvious before critical
- System "looks broken" before it breaks
- No illusion of stability

Frequency: After visual changes, before Stage 2 completion

Ritual 8: The Understanding ≠ Control Test

Purpose: Ensure that full knowledge doesn't enable optimization.

Method:
1. Explain full system dynamics to a user:
   - Stress accumulation/decay rates
   - Burnout thresholds
   - Quality risk mechanics
   - All hidden state
2. Give them perfect visibility (debug mode)
3. Let them attempt to "win" (complete all tasks, no burnout)

Pass condition:
- They still can't prevent all failure
- Perfect knowledge reveals structural insufficiency
- Understanding makes inevitability clearer, not escapable

Fail condition:
- Full knowledge enables perfect play
- System becomes solvable
- Optimization patterns emerge

Frequency: Before each stage completion, after major changes

Ritual 9: The Minimal State Audit

Purpose: Verify that engine state remains minimal and canonical.

Method:
1. List all properties in engine state objects (Task, Person, Project)
2. For each property, ask:
   - Is this required for consequence propagation?
   - Is this display-only data?
   - Is this a derived metric?
   - Could this be removed without breaking simulation?
3. Remove all non-essential properties

Pass condition:
- Only consequence-relevant properties remain
- No display metadata in engine
- No cached/derived values
- State matches File 3 canonical definitions

Fail condition:
- Display data exists in engine state
- Metrics stored for convenience
- Derived values cached

Frequency: After engine changes, before Stage 1 completion

Ritual 10: The Silence Audit

Purpose: Count and eliminate all explanatory elements.

Method:
1. Interact with system for full session
2. Document every instance of:
   - Text that explains "why"
   - Tooltips
   - Warnings
   - Status descriptions
   - Numeric values with labels
3. For each instance, ask: "Does this teach causality?"

Pass condition:
- Zero real-time explanations
- All text is observational, not explanatory
- Numbers (if any) lack context labels

Fail condition:
- Any tooltip explains mechanics
- Warnings predict consequences
- Labels teach understanding

Frequency: Before Stage 2 completion, after any text changes

Ritual 11: The Files 1-3 Compliance Check

Purpose: Verify that implementation still matches conceptual foundations.

Method:
1. Open Files 1, 2, and 3
2. For each invariant, ask:
   - Does current implementation honor this?
   - Has this constraint been violated?
   - Do recent changes respect this?
3. Document all violations
4. Reject or redesign violating changes

Pass condition:
- Zero violations
- All invariants respected
- Recent changes align with foundations

Fail condition:
- Any invariant violated
- Implementation deviates from foundations

Frequency: Before advancing stages, after major changes

Ritual 12: The Stage Boundary Audit

Purpose: Ensure current stage hasn't leaked into future stages.

Method:
1. Review File 4 (Stage Roadmap)
2. List all features/concepts from future stages
3. Check if any have been prematurely implemented
4. Remove all future-stage features

Pass condition:
- Current stage contains only current-stage features
- No anticipatory abstractions exist
- Frozen stages remain unchanged

Fail condition:
- Future features exist in current stage
- Preloaded abstractions present
- Frozen stages modified without justification

Frequency: Before completing each stage

How to Document Calibration Results

For each ritual, record:
- Date performed
- Who performed it
- Pass/fail result
- Specific violations found (if any)
- Actions taken to correct

Maintain a calibration log as evidence of discipline.

Calibration Failure Response

If any ritual fails:
1. Stop - Do not proceed to next stage
2. Document - Record what violated which invariant
3. Trace - Identify which failure mode is emerging
4. Correct - Remove or redesign violating features
5. Re-test - Perform ritual again until pass

Never rationalize calibration failures—fix them.

Anchor Statement

Calibration rituals are not bureaucracy—they are the only defense against slow philosophical drift.

***

**E.7 NON-CALIBRATION (WHAT THIS FRAMEWORK IS NOT)**

Purpose

This section defines what evaluation practices are explicitly rejected—clarifying what this framework does NOT do.

Understanding what evaluation is not prevents drift toward traditional testing mindsets.

This Framework Is NOT Unit Testing

What unit testing does:
- Validates that functions return expected outputs
- Checks edge cases and boundary conditions
- Ensures code correctness
- Measures code coverage

Why this is insufficient:
- A function can work correctly and still violate resistance
- 100% code coverage ≠ philosophical integrity
- Passing unit tests doesn't mean the system feels truthful

Example:
- advanceTime() can work perfectly while the UI adds tooltips that break silence
- Stress calculation can be mathematically correct while recovery feels too fast

This framework evaluates architecture, not code correctness.

This Framework Is NOT Performance Benchmarking

What performance testing does:
- Measures execution speed
- Optimizes for efficiency
- Reduces latency
- Scales for high load

Why this is irrelevant:
- Speed has nothing to do with resistance
- Optimization mindset invites optimization features
- This system will never have "high load"

Example:
- The system running 10x faster doesn't make it more truthful
- Caching stress values for performance creates hidden metrics

Performance optimization is orthogonal to ceiling preservation.

This Framework Is NOT User Acceptance Testing

What UAT does:
- Validates that features match requirements
- Ensures user satisfaction
- Confirms "done" means "what users wanted"
- Measures success against stated goals

Why this is rejected:
- Users are not customers
- Satisfaction is not the goal
- Confusion and discomfort are intentional
- There are no feature requirements to "accept"

Example:
- User saying "I wish this had undo" is validation, not a bug
- User confusion about stress indicates correct implementation

User acceptance would optimize for ease, not truth.

This Framework Is NOT A/B Testing

What A/B testing does:
- Compares variants to maximize conversion
- Optimizes for engagement
- Measures which version performs better
- Drives toward higher metrics

Why this is toxic:
- More engaging ≠ more truthful
- A/B testing creates pressure to make the system easier
- Metrics-driven decisions erode resistance

Example:
- Version A (with warnings) might have higher completion rates
- Version B (silent) might frustrate users more
- A/B testing would choose Version A, which violates the ceiling

Engagement optimization is antithetical to resistance.

This Framework Is NOT Accessibility Compliance

What accessibility testing does:
- Ensures WCAG compliance
- Validates screen reader compatibility
- Checks color contrast ratios
- Ensures keyboard navigation

Why this is different:
- Accessibility = perceivable, not preventable
- Making consequences observable ≠ making them avoidable
- Compliance checklists don't evaluate resistance

Example:
- Screen reader should describe visual state (task appears degraded)
- Screen reader should NOT explain why (because person was overloaded)

Accessibility ensures observability, not controllability.

This Framework Is NOT Bug Tracking

What bug tracking does:
- Documents defects
- Prioritizes fixes
- Tracks completion rates
- Measures quality as "bugs per feature"

Why this is incomplete:
- Philosophical violations are not bugs
- A "working" feature can still break the ceiling
- Some "bugs" (like confusion) are correct behavior

Example:
- User reports: "Burnout happened without warning" → Not a bug, working as intended
- User reports: "I can't see stress levels" → Not a bug, violates silence

This framework identifies ceiling violations, not code defects.

This Framework Is NOT Usability Testing

What usability testing does:
- Measures task completion rates
- Identifies points of confusion
- Optimizes for ease of use
- Reduces friction

Why this is opposed:
- Ease of use ≠ truthfulness
- Reducing confusion weakens resistance
- Friction is structural, not accidental

Example:
- Usability test might say "Users can't find the undo button"
- Correct response: "There is no undo button, and never will be"

Usability optimization would destroy resistance.

This Framework Is NOT Heuristic Evaluation (Traditional)

What traditional heuristic evaluation does:
- Applies Nielsen's 10 usability heuristics
- Checks for visibility of system status
- Ensures error prevention
- Validates user control and freedom

Why this is inverted:
- Visibility of system status → Violates silence
- Error prevention → Prevents consequence
- User control → Weakens irreversibility

Example:
- Nielsen says "Make system status visible" → This system hides stress
- Nielsen says "Provide undo/redo" → This system forbids it

This framework uses opposite heuristics: silence, resistance, irreversibility.

This Framework Is NOT Feature Completeness Tracking

What feature tracking does:
- Measures progress against roadmap
- Tracks percent complete
- Validates all requirements implemented
- Ensures nothing is "missing"

Why this is irrelevant:
- Features can be "complete" and still violate the ceiling
- Adding more features often weakens the system
- "Missing" features might be intentionally absent

Example:
- Roadmap says Stage 3 is "complete" but interactions feel too powerful
- Feature checklist says "explainability done" but it appears during execution

Completeness is stage-based (File 4), not feature-based.

This Framework Is NOT Competitive Analysis

What competitive analysis does:
- Compares features to similar products
- Identifies gaps
- Recommends parity features
- Benchmarks against industry standards

Why this is toxic:
- This system doesn't compete with products
- Feature parity would destroy resistance
- Industry standards optimize for sales, not truth

Example:
- "Jira shows burndown charts, we should too" → Violates silence
- "Linear has undo, users expect it" → Violates irreversibility

This system models different truths than products optimize for.

This Framework Is NOT Regression Testing

What regression testing does:
- Ensures new changes don't break old features
- Validates that fixes don't introduce new bugs
- Maintains feature stability

Why this is insufficient:
- Can pass all regression tests while adding helpful tooltips
- Doesn't detect philosophical drift
- Focuses on code, not architecture

Example:
- Regression tests pass after adding stress meter
- But stress meter violates silence principle

This framework detects architectural regression, not code regression.

This Framework Is NOT Conversion Funnel Optimization

What funnel optimization does:
- Measures user drop-off points
- Optimizes for retention
- Maximizes completion rates
- Reduces abandonment

Why this is opposed:
- Abandonment might indicate correct difficulty
- Retention ≠ truthfulness
- Completion optimization teaches avoidance

Example:
- Funnel analysis shows "users quit after first burnout"
- Optimization response: "Add recovery mechanic"
- Correct response: "Burnout is working as intended"

This system has no conversion goal.
This Framework Is NOT Customer Satisfaction Measurement
What satisfaction testing does:

NPS scores

User satisfaction surveys

Feature request voting

Sentiment analysis

Why this is irrelevant:

Users are not customers

Satisfaction is not the goal

Feature requests might violate the ceiling

Positive sentiment ≠ truthful system

Example:

Survey says "Users want stress indicators" → Rejected, violates silence

NPS is low because "no undo button" → Validation, not a problem

This is an artifact, not a product seeking satisfaction.

This Framework Is NOT Standards Compliance
What standards compliance does:

ISO certification

Industry best practices

Design pattern conformance

Framework conventions

Why this is irrelevant:

Industry standards optimize for different goals

Best practices might violate resistance

Conventions assume product mindset

Example:

Design pattern says "Always provide feedback on user actions"

This system deliberately withholds feedback

This framework defines its own standards (Files 1-3).

What This Framework Actually Is
This framework is:

Philosophical integrity testing

Ceiling preservation validation

Resistance calibration

Truth-first evaluation

Architectural alignment checking

It asks: "Does this system remain truthful?" not "Does this system work well?"

The Core Distinction
Traditional evaluation asks:

Does it work?

Do users like it?

Is it complete?

Is it fast enough?

This framework asks:

Does resistance hold?

Is irreversibility intact?

Is silence preserved?

Does history still dominate?

These are fundamentally different questions.

Anchor Statement
This framework evaluates whether the system is truthful, not whether it is good—truth and goodness often conflict, and truth always wins.

End of E.7

---

